{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", tf.config.list_physical_devices())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. No outlier treatment\n",
    "Modelos entrenados sin tratamiento de outliers, hay dos casos:\n",
    "- Datos normalizados entre 0 y 1\n",
    "- Datos normalizados entre -1 y 1\n",
    "\n",
    "Los modelos en la carpeta models se identifican de la siguiente forma:\n",
    "\n",
    "{estación}\\_{ventana temporal}\\_{numero de outputs}\\_{batch size}\\_{epochs}\\_{drop out}\\_{neuronas}\\_{optimizer}\\_{normalization}.h5\n",
    "\n",
    "Por ejemplo: **C6_24_1_6_20_0.05_64_adam_-11.h5**\n",
    "- Datos de la estación C6\n",
    "- La ventana temporal es de 24 (24 datos previos al instante predicho, cada una de estas 24 representa una medición cada 30min)\n",
    "- Solo hay una salida (5 valores pero solo 30 minutos)\n",
    "- Batch size de 6.\n",
    "- Entrenado durante 20 epochs\n",
    "- Drop out de 0.05\n",
    "- La capa LSTM tiene 64 neuronas.\n",
    "- El optimizador es ADAM\n",
    "- Los datos son normalizados entre -1 y 1. \n",
    "\n",
    "\n",
    "\n",
    "## 1.1. Training phase\n",
    "In here the training parameters for the sesion are decided. A list of dicts that contains the parameters of each model is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_width = [48]\n",
    "prediction_width = [1]\n",
    "batch_size = [6]\n",
    "epochs = [10]\n",
    "dropout = [0.05]\n",
    "neurons = [64]\n",
    "optimizer = ['adagrad']\n",
    "normalization = [[-1, 1]]#, [-1, 1]]\n",
    "\n",
    "station = 'C6.zip'\n",
    "variables = ['T', 'HR', 'P', 'u10', 'v10', 'day', 'time', 'date']\n",
    "input_vars = ['T', 'HR', 'P', 'u10', 'v10', 'day', 'time']\n",
    "cols = ['T', 'HR', 'P', 'u10', 'v10']\n",
    "\n",
    "df_initial = pd.read_csv(f'data/data_by_station/{station}', compression='zip', header=0, sep=',')\n",
    "df_initial['date'] = pd.to_datetime(df_initial['date'], format='%Y-%m-%d %H:%M:%S')\n",
    "df_initial['day'] = df_initial['date'].dt.dayofyear / 365\n",
    "df_initial['time'] = df_initial['date'].dt.hour / 24\n",
    "df_initial = df_initial.astype({'T': 'float', 'HR': 'float', 'P': 'float', 'u2': 'float', 'v2': 'float', 'u6': 'float', 'v6': 'float', 'u10': 'float', 'v10': 'float', 'altitud': 'float', 'latitud': 'float', 'longitud': 'float'})\n",
    "df_initial = df_initial[variables]\n",
    "\n",
    "parameters = []\n",
    "for i in input_width:\n",
    "    for j in prediction_width:\n",
    "        for k in batch_size:\n",
    "            for l in epochs:\n",
    "                for m in dropout:\n",
    "                    for n in neurons:\n",
    "                            for p in optimizer:\n",
    "                                for s in normalization:\n",
    "                                    parameters.append({'width':i, 'output': j, 'batch': k, 'epochs': l, 'dropout': m, 'neurons': n, 'opt': p, 'norm': s})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "23659/23659 [==============================] - 164s 7ms/step - loss: 0.0230 - mae: 0.1101 - val_loss: 0.0130 - val_mae: 0.0782\n",
      "Epoch 2/10\n",
      "23659/23659 [==============================] - 163s 7ms/step - loss: 0.0138 - mae: 0.0823 - val_loss: 0.0117 - val_mae: 0.0732\n",
      "Epoch 3/10\n",
      "23659/23659 [==============================] - 163s 7ms/step - loss: 0.0129 - mae: 0.0790 - val_loss: 0.0112 - val_mae: 0.0710\n",
      "Epoch 4/10\n",
      "23659/23659 [==============================] - 165s 7ms/step - loss: 0.0124 - mae: 0.0769 - val_loss: 0.0108 - val_mae: 0.0694\n",
      "Epoch 5/10\n",
      "23659/23659 [==============================] - 160s 7ms/step - loss: 0.0121 - mae: 0.0755 - val_loss: 0.0106 - val_mae: 0.0682\n",
      "Epoch 6/10\n",
      "23659/23659 [==============================] - 165s 7ms/step - loss: 0.0118 - mae: 0.0744 - val_loss: 0.0104 - val_mae: 0.0673\n",
      "Epoch 7/10\n",
      "23659/23659 [==============================] - 165s 7ms/step - loss: 0.0116 - mae: 0.0735 - val_loss: 0.0102 - val_mae: 0.0665\n",
      "Epoch 8/10\n",
      "23659/23659 [==============================] - 165s 7ms/step - loss: 0.0114 - mae: 0.0727 - val_loss: 0.0101 - val_mae: 0.0658\n",
      "Epoch 9/10\n",
      "23659/23659 [==============================] - 170s 7ms/step - loss: 0.0112 - mae: 0.0720 - val_loss: 0.0099 - val_mae: 0.0653\n",
      "Epoch 10/10\n",
      "23651/23659 [============================>.] - ETA: 0s - loss: 0.0111 - mae: 0.0715"
     ]
    }
   ],
   "source": [
    "last_params = {'width': 0, 'norm': []}\n",
    "for params in parameters:\n",
    "    df = df_initial.copy()\n",
    "    if params['norm'] != [0, 0]:\n",
    "        for col in cols:\n",
    "            df[col] = ((params['norm'][1] - params['norm'][0]) * (df[col] - df_initial[col].min()) / (df_initial[col].max() - df_initial[col].min())) + params['norm'][0]\n",
    "\n",
    "    df_train = df[df['date'] < '2019-01-01'].copy()\n",
    "    df_test = df[(df['date'] >= '2019-01-01') & (df['date'] <= '2022-12-31')].copy()\n",
    "\n",
    "    train_X = []\n",
    "    train_Y = []\n",
    "    for i in range(params['width'], len(df_train) - params['output']):\n",
    "        train_X.append(df_train.iloc[i - params['width']:i][input_vars].values)\n",
    "        train_Y.append(df_train.iloc[i:i + params['output']][cols].values)\n",
    "    train_X = np.array(train_X)\n",
    "    train_Y = np.array(train_Y)\n",
    "\n",
    "    test_X = []\n",
    "    test_Y = []\n",
    "    for i in range(params['width'], len(df_test) - params['output']):\n",
    "        test_X.append(df_test.iloc[i - params['width']:i][input_vars].values)\n",
    "        test_Y.append(df_test.iloc[i:i + params['output']][cols].values)\n",
    "    test_X = np.array(test_X)\n",
    "    test_Y = np.array(test_Y)\n",
    "        \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(params['neurons'], activation='tanh', input_shape=(train_X.shape[1], train_X.shape[2]), return_sequences=False))\n",
    "    model.add(Dropout(params['dropout']))\n",
    "    model.add(Dense(units=len(cols), activation='linear'))\n",
    "    model.compile(optimizer=params['opt'], loss='mse', metrics=['mae'])\n",
    "\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        history = model.fit(train_X, train_Y, epochs=params['epochs'], batch_size=params['batch'], validation_split=0.1, verbose=1, shuffle=False)\n",
    "    model.save(f'models/{station.strip(\".zip\")}_{params[\"width\"]}_{params[\"output\"]}_{params[\"batch\"]}_{params[\"epochs\"]}_{params[\"dropout\"]}_{params[\"neurons\"]}_{params[\"opt\"]}_{params[\"norm\"][0]}{params[\"norm\"][1]}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.system('shutdown -s -t 0')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Prediction and plotting phase \n",
    "In this phase the predictions of the models trained are done and saved to plot afterwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "546/546 [==============================] - 2s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "directories = os.listdir('models')\n",
    "\n",
    "norm = [model for model in directories if 'h5' in model]\n",
    "\n",
    "results = []\n",
    "\n",
    "for model_path in norm:\n",
    "    params = model_path.strip('.h5').split('_')\n",
    "    if '01' in params[-1]:\n",
    "        norm_min, norm_max = 0, 1\n",
    "    else:\n",
    "        norm_min, norm_max = -1, 1\n",
    "    df_test = df_initial[df_initial['date'] >= '2019-01-01'].copy()\n",
    "\n",
    "    for col in cols:\n",
    "        df_test[col] = ((norm_max - norm_min) * (df_test[col] - df_initial[col].min()) / (df_initial[col].max() - df_initial[col].min())) + norm_min\n",
    "\n",
    "    test_X = []\n",
    "    test_Y = []\n",
    "    for i in range(int(params[1]), len(df_test) - int(params[2])):\n",
    "        test_X.append(df_test.iloc[i - int(params[1]):i][input_vars].values)\n",
    "        test_Y.append(df_test.iloc[i:i + int(params[2])][cols].values)\n",
    "    test_X = np.array(test_X)\n",
    "    test_Y = np.array(test_Y)\n",
    "\n",
    "    model = keras.models.load_model(f'models/{model_path}')\n",
    "    y_pred = model.predict(test_X)\n",
    "    for idx, col in enumerate(cols):\n",
    "        y_pred[:, idx] = ((y_pred[:, idx] - norm_min) * (df_initial[col].max() - df_initial[col].min()) / (norm_max - norm_min)) + df_initial[col].min()\n",
    "    \n",
    "    results.append(y_pred)\n",
    "    \n",
    "for idx, col in enumerate(cols):\n",
    "    test_Y[:, 0, idx] = ((test_Y[:, 0, idx] - norm_min) * (df_initial[col].max() - df_initial[col].min()) / (norm_max - norm_min)) + df_initial[col].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "minimum = np.inf\n",
    "for idx, result in enumerate(results):\n",
    "    if len(result) < minimum:\n",
    "        minimum = len(result)\n",
    "for idx, result in enumerate(results):\n",
    "    results[idx] = result[-minimum:]\n",
    "test_Y = test_Y[-minimum:, 0, :]\n",
    "start = 350\n",
    "for idx, col in enumerate(cols):\n",
    "    # plot each column\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(test_Y[start:start+480, idx], label='Real', color='blue')\n",
    "    for idx2, result in enumerate(results):\n",
    "        params = norm[idx2].strip('.h5').split('_')\n",
    "        rmse = np.sqrt(np.mean(np.power((test_Y[:, idx] - result[:, idx]), 2)))\n",
    "        mae = np.mean(np.abs(test_Y[:, idx] - result[:, idx]))\n",
    "        plt.plot(result[start:start+480, idx], label=f'NORM {params[-1]}, RMSE: {rmse:.4f}, MAE: {mae:.4f}')\n",
    "    plt.legend()\n",
    "    plt.title(f'Starting model vs Final model - {params[4]} Normalization')\n",
    "    plt.savefig(f'plots/{col}_nada.png')\n",
    "    plt.clf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C6', '48', '1', '16', '15', '0.2', '8', 'adam', '-11']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Treated outliers\n",
    "\n",
    "Modelos entrenados usando IQR como tratamiento de outliers, hay dos casos:\n",
    "- Datos normalizados entre 0 y 1\n",
    "- Datos normalizados entre -1 y 1\n",
    "\n",
    "Los modelos en la carpeta models se identifican de la siguiente forma:\n",
    "\n",
    "{estación}\\_{ventana temporal}\\_{numero de outputs}\\_{batch size}\\_{epochs}\\_{drop out}\\_{neuronas}\\_{optimizer}\\_{normalization}.h5\n",
    "\n",
    "Por ejemplo: **C6_24_1_6_20_0.05_64_adam_o-11.h5**\n",
    "- Datos de la estación C6\n",
    "- La ventana temporal es de 24 (24 datos previos al instante predicho, cada una de estas 24 representa una medición cada 30min)\n",
    "- Solo hay una salida (5 valores pero solo 30 minutos)\n",
    "- Batch size de 6.\n",
    "- Entrenado durante 20 epochs\n",
    "- Drop out de 0.05\n",
    "- La capa LSTM tiene 64 neuronas.\n",
    "- El optimizador es ADAM\n",
    "- Los datos son normalizados entre -1 y 1 y con los valores extremos tratados usando IQR. \n",
    "\n",
    "\n",
    "\n",
    "## 2.1. Training phase\n",
    "In here the training parameters for the sesion are decided. A list of dicts that contains the parameters of each model is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_width = [24]\n",
    "prediction_width = [1]\n",
    "batch_size = [6]\n",
    "epochs = [15]\n",
    "dropout = [0.05]\n",
    "neurons = [64]\n",
    "optimizer = ['adam']\n",
    "normalization = [[-1, 1]]#, [0, 1]]\n",
    "\n",
    "station = 'C6.zip'\n",
    "variables = ['T', 'HR', 'P', 'u10', 'v10', 'day', 'time', 'date']\n",
    "input_vars = ['T', 'HR', 'P', 'u10', 'v10', 'day', 'time']\n",
    "cols = ['T', 'HR', 'P', 'u10', 'v10']\n",
    "\n",
    "df_initial = pd.read_csv(f'data/data_by_station/{station}', compression='zip', header=0, sep=',')\n",
    "df_initial['date'] = pd.to_datetime(df_initial['date'], format='%Y-%m-%d %H:%M:%S')\n",
    "df_initial['day'] = df_initial['date'].dt.dayofyear / 365\n",
    "df_initial['time'] = df_initial['date'].dt.hour / 24\n",
    "df_initial = df_initial.astype({'T': 'float', 'HR': 'float', 'P': 'float', 'u2': 'float', 'v2': 'float', 'u6': 'float', 'v6': 'float', 'u10': 'float', 'v10': 'float', 'altitud': 'float', 'latitud': 'float', 'longitud': 'float'})\n",
    "df_initial = df_initial[variables]\n",
    "\n",
    "parameters = []\n",
    "for i in input_width:\n",
    "    for j in prediction_width:\n",
    "        for k in batch_size:\n",
    "            for l in epochs:\n",
    "                for m in dropout:\n",
    "                    for n in neurons:\n",
    "                            for p in optimizer:\n",
    "                                for s in normalization:\n",
    "                                    parameters.append({'width':i, 'output': j, 'batch': k, 'epochs': l, 'dropout': m, 'neurons': n, 'opt': p, 'norm': s})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "17747/17747 [==============================] - 151s 8ms/step - loss: 0.0248 - mae: 0.1027 - val_loss: 0.0258 - val_mae: 0.1075\n",
      "Epoch 2/15\n",
      "17747/17747 [==============================] - 146s 8ms/step - loss: 0.0227 - mae: 0.0967 - val_loss: 0.0274 - val_mae: 0.1111\n",
      "Epoch 3/15\n",
      "17747/17747 [==============================] - 148s 8ms/step - loss: 0.0220 - mae: 0.0951 - val_loss: 0.0254 - val_mae: 0.1060\n",
      "Epoch 4/15\n",
      "17747/17747 [==============================] - 147s 8ms/step - loss: 0.0216 - mae: 0.0939 - val_loss: 0.0247 - val_mae: 0.1036\n",
      "Epoch 5/15\n",
      "17747/17747 [==============================] - 148s 8ms/step - loss: 0.0212 - mae: 0.0930 - val_loss: 0.0241 - val_mae: 0.1023\n",
      "Epoch 6/15\n",
      "17747/17747 [==============================] - 148s 8ms/step - loss: 0.0209 - mae: 0.0923 - val_loss: 0.0236 - val_mae: 0.1007\n",
      "Epoch 7/15\n",
      "17747/17747 [==============================] - 149s 8ms/step - loss: 0.0207 - mae: 0.0916 - val_loss: 0.0232 - val_mae: 0.0990\n",
      "Epoch 8/15\n",
      "17747/17747 [==============================] - 148s 8ms/step - loss: 0.0204 - mae: 0.0910 - val_loss: 0.0231 - val_mae: 0.0995\n",
      "Epoch 9/15\n",
      "17747/17747 [==============================] - 147s 8ms/step - loss: 0.0202 - mae: 0.0904 - val_loss: 0.0233 - val_mae: 0.1000\n",
      "Epoch 10/15\n",
      "17747/17747 [==============================] - 148s 8ms/step - loss: 0.0200 - mae: 0.0900 - val_loss: 0.0236 - val_mae: 0.1002\n",
      "Epoch 11/15\n",
      "17747/17747 [==============================] - 148s 8ms/step - loss: 0.0199 - mae: 0.0896 - val_loss: 0.0242 - val_mae: 0.1019\n",
      "Epoch 12/15\n",
      "17747/17747 [==============================] - 147s 8ms/step - loss: 0.0197 - mae: 0.0893 - val_loss: 0.0237 - val_mae: 0.1001\n",
      "Epoch 13/15\n",
      "17747/17747 [==============================] - 149s 8ms/step - loss: 0.0196 - mae: 0.0890 - val_loss: 0.0238 - val_mae: 0.1002\n",
      "Epoch 14/15\n",
      "17747/17747 [==============================] - 148s 8ms/step - loss: 0.0196 - mae: 0.0888 - val_loss: 0.0238 - val_mae: 0.1003\n",
      "Epoch 15/15\n",
      "17747/17747 [==============================] - 148s 8ms/step - loss: 0.0195 - mae: 0.0886 - val_loss: 0.0235 - val_mae: 0.0999\n"
     ]
    }
   ],
   "source": [
    "last_params = {'width': 0, 'norm': []}\n",
    "min_maxs = {}\n",
    "\n",
    "for params in parameters:\n",
    "    df = df_initial.copy()\n",
    "    for col in cols:\n",
    "        iqr = df_initial[col].quantile(0.75) - df_initial[col].quantile(0.25)\n",
    "        min_maxs[col] = [df_initial[col].quantile(0.25) - 1.5 * iqr, df_initial[col].quantile(0.75) + 1.5 * iqr]\n",
    "        df[col] = ((params['norm'][1] - params['norm'][0]) * (df[col] - min_maxs[col][0]) / (min_maxs[col][1] - min_maxs[col][0])) + params['norm'][0]\n",
    "        df.loc[df[col] < params['norm'][0], col] = params['norm'][0]\n",
    "        df.loc[df[col] > params['norm'][1], col] = params['norm'][1]\n",
    "        \n",
    "    df_train = df[df['date'] < '2019-01-01'].copy()\n",
    "    df_test = df[df['date'] >= '2019-01-01'].copy()\n",
    "\n",
    "    train_X = []\n",
    "    train_Y = []\n",
    "    for i in range(params['width'], len(df_train) - params['output']):\n",
    "        train_X.append(df_train.iloc[i - params['width']:i][input_vars].values)\n",
    "        train_Y.append(df_train.iloc[i:i + params['output']][cols].values)\n",
    "    train_X = np.array(train_X)\n",
    "    train_Y = np.array(train_Y)\n",
    "\n",
    "    test_X = []\n",
    "    test_Y = []\n",
    "    for i in range(params['width'], len(df_test) - params['output']):\n",
    "        test_X.append(df_test.iloc[i - params['width']:i][input_vars].values)\n",
    "        test_Y.append(df_test.iloc[i:i + params['output']][cols].values)\n",
    "    test_X = np.array(test_X)\n",
    "    test_Y = np.array(test_Y)\n",
    "        \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(params['neurons'], activation='tanh', input_shape=(train_X.shape[1], train_X.shape[2]), return_sequences=False))\n",
    "    model.add(Dropout(params['dropout']))\n",
    "    model.add(Dense(units=len(cols), activation='linear'))\n",
    "    model.compile(optimizer=params['opt'], loss='mse', metrics=['mae'])\n",
    "\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        history = model.fit(train_X, train_Y, epochs=params['epochs'], batch_size=params['batch'], validation_split=0.1, verbose=1, shuffle=False)\n",
    "    model.save(f'models/{station.strip(\".zip\")}_{params[\"width\"]}_{params[\"output\"]}_{params[\"batch\"]}_{params[\"epochs\"]}_{params[\"dropout\"]}_{params[\"neurons\"]}_{params[\"opt\"]}_{params[\"norm\"][0]}{params[\"norm\"][1]}.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Prediction and plotting phase \n",
    "In this phase the predictions of the models trained are done and saved to plot afterwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "547/547 [==============================] - 2s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "directories = os.listdir('models')\n",
    "\n",
    "norm = [model for model in directories if '11' in model]\n",
    "\n",
    "results = []\n",
    "\n",
    "for model_path in norm:\n",
    "    params = model_path.strip('.h5').split('_')\n",
    "    if '01' in params[-1]:\n",
    "        norm_min, norm_max = 0, 1\n",
    "    else:\n",
    "        norm_min, norm_max = -1, 1\n",
    "    df_test = df_initial[df_initial['date'] >= '2019-01-01'].copy()\n",
    "\n",
    "    for col in cols:\n",
    "        iqr = df_initial[col].quantile(0.75) - df_initial[col].quantile(0.25)\n",
    "        min_maxs[col] = [df_initial[col].quantile(0.25) - 1.5 * iqr, df_initial[col].quantile(0.75) + 1.5 * iqr]\n",
    "        df_test[col] = ((norm_max - norm_min) * (df_test[col] - min_maxs[col][0]) / (min_maxs[col][1] - min_maxs[col][0])) + norm_min\n",
    "        df_test.loc[df_test[col] < norm_min, col] = norm_min\n",
    "        df_test.loc[df_test[col] > norm_max, col] = norm_max\n",
    "    \n",
    "    test_X = []\n",
    "    test_Y = []\n",
    "    for i in range(int(params[1]), len(df_test) - int(params[2])):\n",
    "        test_X.append(df_test.iloc[i - int(params[1]):i][input_vars].values)\n",
    "        test_Y.append(df_test.iloc[i:i + int(params[2])][cols].values)\n",
    "    test_X = np.array(test_X)\n",
    "    test_Y = np.array(test_Y)\n",
    "\n",
    "    model = keras.models.load_model(f'models/{model_path}')\n",
    "    y_pred = model.predict(test_X)\n",
    "    for idx, col in enumerate(cols):\n",
    "        y_pred[:, idx] = ((y_pred[:, idx] - norm_min) * (df_initial[col].max() - df_initial[col].min()) / (norm_max - norm_min)) + df_initial[col].min()\n",
    "    \n",
    "    results.append(y_pred)\n",
    "\n",
    "for idx, col in enumerate(cols):\n",
    "    test_Y[:, 0, idx] = ((test_Y[:, 0, idx] - norm_min) * (df_initial[col].max() - df_initial[col].min()) / (norm_max - norm_min)) + df_initial[col].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "minimum = np.inf\n",
    "for idx, result in enumerate(results):\n",
    "    if len(result) < minimum:\n",
    "        minimum = len(result)\n",
    "for idx, result in enumerate(results):\n",
    "    results[idx] = result[-minimum:]\n",
    "test_Y = test_Y[-minimum:, 0, :]\n",
    "\n",
    "for idx, col in enumerate(cols):\n",
    "    # plot each column\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(test_Y[:, idx], label='Real', color='blue')\n",
    "    for idx2, result in enumerate(results):\n",
    "        params = norm[idx2].strip('.h5').split('_')\n",
    "        rmse = np.sqrt(np.mean(np.power((test_Y[:, idx] - result[:, idx]), 2)))\n",
    "        plt.plot(result[:, idx], label=f'RMSE {rmse:.4f}', color='g')\n",
    "    plt.legend()\n",
    "    plt.title(f'{col} - Drop out - {params[-1]} Normalization (No outliers)')\n",
    "    plt.savefig(f'plots/{col}_dropOut_o{params[-1]}norm.png')\n",
    "    plt.clf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
