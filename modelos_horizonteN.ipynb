{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", tf.config.list_physical_devices())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. No outlier treatment\n",
    "Modelos entrenados sin tratamiento de outliers, hay dos casos:\n",
    "- Datos normalizados entre 0 y 1\n",
    "- Datos normalizados entre -1 y 1\n",
    "\n",
    "Los modelos en la carpeta models se identifican de la siguiente forma:\n",
    "\n",
    "{estación}\\_{ventana temporal}\\_{numero de outputs}\\_{batch size}\\_{epochs}\\_{drop out}\\_{neuronas}\\_{optimizer}\\_{normalization}.h5\n",
    "\n",
    "Por ejemplo: **C6_24_1_6_20_0.05_64_adam_-11.h5**\n",
    "- Datos de la estación C6\n",
    "- La ventana temporal es de 24 (24 datos previos al instante predicho, cada una de estas 24 representa una medición cada 30min)\n",
    "- Solo hay una salida (5 valores pero solo 30 minutos)\n",
    "- Batch size de 6.\n",
    "- Entrenado durante 20 epochs\n",
    "- Drop out de 0.05\n",
    "- La capa LSTM tiene 64 neuronas.\n",
    "- El optimizador es ADAM\n",
    "- Los datos son normalizados entre -1 y 1. \n",
    "\n",
    "\n",
    "\n",
    "## 1.1. Training phase\n",
    "In here the training parameters for the sesion are decided. A list of dicts that contains the parameters of each model is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_width = [24]\n",
    "prediction_width = [4, 8, 12, 16]\n",
    "batch_size = [4]\n",
    "epochs = [15]\n",
    "dropout = [0.05]\n",
    "neurons = [64]\n",
    "optimizer = ['adam']\n",
    "normalization = [[-1, 1]]#, [0, 1]]\n",
    "\n",
    "station = 'C6.zip'\n",
    "variables = ['T', 'HR', 'P', 'u10', 'v10', 'day', 'time', 'date']\n",
    "input_vars = ['T', 'HR', 'P', 'u10', 'v10', 'day', 'time']\n",
    "cols = ['T', 'HR', 'P', 'u10', 'v10']\n",
    "\n",
    "df_initial = pd.read_csv(f'data/data_by_station/{station}', compression='zip', header=0, sep=',')\n",
    "df_initial['date'] = pd.to_datetime(df_initial['date'], format='%Y-%m-%d %H:%M:%S')\n",
    "df_initial['day'] = df_initial['date'].dt.dayofyear / 365\n",
    "df_initial['time'] = df_initial['date'].dt.hour / 24\n",
    "df_initial = df_initial.astype({'T': 'float', 'HR': 'float', 'P': 'float', 'u2': 'float', 'v2': 'float', 'u6': 'float', 'v6': 'float', 'u10': 'float', 'v10': 'float', 'altitud': 'float', 'latitud': 'float', 'longitud': 'float'})\n",
    "df_initial = df_initial[variables]\n",
    "\n",
    "parameters = []\n",
    "for i in input_width:\n",
    "    for j in prediction_width:\n",
    "        for k in batch_size:\n",
    "            for l in epochs:\n",
    "                for m in dropout:\n",
    "                    for n in neurons:\n",
    "                            for p in optimizer:\n",
    "                                for s in normalization:\n",
    "                                    parameters.append({'width':i, 'output': j, 'batch': k, 'epochs': l, 'dropout': m, 'neurons': n, 'opt': p, 'norm': s})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "35494/35494 [==============================] - 309s 8ms/step - loss: 0.0081 - mae: 0.0610 - val_loss: 0.0132 - val_mae: 0.0802\n",
      "Epoch 2/15\n",
      "35494/35494 [==============================] - 303s 9ms/step - loss: 0.0064 - mae: 0.0532 - val_loss: 0.0126 - val_mae: 0.0765\n",
      "Epoch 3/15\n",
      "35494/35494 [==============================] - 301s 8ms/step - loss: 0.0062 - mae: 0.0519 - val_loss: 0.0124 - val_mae: 0.0766\n",
      "Epoch 4/15\n",
      "35494/35494 [==============================] - 306s 9ms/step - loss: 0.0061 - mae: 0.0513 - val_loss: 0.0115 - val_mae: 0.0738\n",
      "Epoch 5/15\n",
      "35494/35494 [==============================] - 304s 9ms/step - loss: 0.0060 - mae: 0.0508 - val_loss: 0.0114 - val_mae: 0.0735\n",
      "Epoch 6/15\n",
      "35494/35494 [==============================] - 301s 8ms/step - loss: 0.0059 - mae: 0.0504 - val_loss: 0.0110 - val_mae: 0.0721\n",
      "Epoch 7/15\n",
      "35494/35494 [==============================] - 303s 9ms/step - loss: 0.0059 - mae: 0.0502 - val_loss: 0.0107 - val_mae: 0.0715\n",
      "Epoch 8/15\n",
      "35494/35494 [==============================] - 302s 9ms/step - loss: 0.0058 - mae: 0.0500 - val_loss: 0.0109 - val_mae: 0.0718\n",
      "Epoch 9/15\n",
      "35494/35494 [==============================] - 304s 9ms/step - loss: 0.0058 - mae: 0.0498 - val_loss: 0.0105 - val_mae: 0.0708\n",
      "Epoch 10/15\n",
      "35494/35494 [==============================] - 301s 8ms/step - loss: 0.0058 - mae: 0.0497 - val_loss: 0.0101 - val_mae: 0.0690\n",
      "Epoch 11/15\n",
      "35494/35494 [==============================] - 307s 9ms/step - loss: 0.0057 - mae: 0.0495 - val_loss: 0.0103 - val_mae: 0.0707\n",
      "Epoch 12/15\n",
      "35494/35494 [==============================] - 302s 9ms/step - loss: 0.0057 - mae: 0.0494 - val_loss: 0.0100 - val_mae: 0.0691\n",
      "Epoch 13/15\n",
      "35494/35494 [==============================] - 305s 9ms/step - loss: 0.0057 - mae: 0.0493 - val_loss: 0.0098 - val_mae: 0.0685\n",
      "Epoch 14/15\n",
      "35494/35494 [==============================] - 302s 8ms/step - loss: 0.0057 - mae: 0.0492 - val_loss: 0.0094 - val_mae: 0.0667\n",
      "Epoch 15/15\n",
      "35494/35494 [==============================] - 302s 9ms/step - loss: 0.0056 - mae: 0.0491 - val_loss: 0.0094 - val_mae: 0.0674\n",
      "Epoch 1/15\n",
      "35493/35493 [==============================] - 298s 8ms/step - loss: 0.0120 - mae: 0.0749 - val_loss: 0.0270 - val_mae: 0.1084\n",
      "Epoch 2/15\n",
      "35493/35493 [==============================] - 294s 8ms/step - loss: 0.0097 - mae: 0.0661 - val_loss: 0.0241 - val_mae: 0.1010\n",
      "Epoch 3/15\n",
      "35493/35493 [==============================] - 296s 8ms/step - loss: 0.0093 - mae: 0.0643 - val_loss: 0.0218 - val_mae: 0.0953\n",
      "Epoch 4/15\n",
      "35493/35493 [==============================] - 296s 8ms/step - loss: 0.0091 - mae: 0.0633 - val_loss: 0.0197 - val_mae: 0.0911\n",
      "Epoch 5/15\n",
      "35493/35493 [==============================] - 293s 8ms/step - loss: 0.0089 - mae: 0.0627 - val_loss: 0.0178 - val_mae: 0.0873\n",
      "Epoch 6/15\n",
      "35493/35493 [==============================] - 292s 8ms/step - loss: 0.0088 - mae: 0.0622 - val_loss: 0.0166 - val_mae: 0.0851\n",
      "Epoch 7/15\n",
      "35493/35493 [==============================] - 294s 8ms/step - loss: 0.0088 - mae: 0.0619 - val_loss: 0.0157 - val_mae: 0.0828\n",
      "Epoch 8/15\n",
      "35493/35493 [==============================] - 294s 8ms/step - loss: 0.0087 - mae: 0.0616 - val_loss: 0.0148 - val_mae: 0.0806\n",
      "Epoch 9/15\n",
      "35493/35493 [==============================] - 297s 8ms/step - loss: 0.0086 - mae: 0.0614 - val_loss: 0.0139 - val_mae: 0.0790\n",
      "Epoch 10/15\n",
      "35493/35493 [==============================] - 294s 8ms/step - loss: 0.0086 - mae: 0.0612 - val_loss: 0.0138 - val_mae: 0.0790\n",
      "Epoch 11/15\n",
      "35493/35493 [==============================] - 296s 8ms/step - loss: 0.0086 - mae: 0.0611 - val_loss: 0.0132 - val_mae: 0.0771\n",
      "Epoch 12/15\n",
      "35493/35493 [==============================] - 298s 8ms/step - loss: 0.0085 - mae: 0.0609 - val_loss: 0.0130 - val_mae: 0.0768\n",
      "Epoch 13/15\n",
      "35493/35493 [==============================] - 298s 8ms/step - loss: 0.0085 - mae: 0.0608 - val_loss: 0.0126 - val_mae: 0.0756\n",
      "Epoch 14/15\n",
      "35493/35493 [==============================] - 294s 8ms/step - loss: 0.0085 - mae: 0.0607 - val_loss: 0.0123 - val_mae: 0.0741\n",
      "Epoch 15/15\n",
      "35493/35493 [==============================] - 296s 8ms/step - loss: 0.0084 - mae: 0.0605 - val_loss: 0.0122 - val_mae: 0.0748\n",
      "Epoch 1/15\n",
      "35492/35492 [==============================] - 299s 8ms/step - loss: 0.0150 - mae: 0.0847 - val_loss: 0.0385 - val_mae: 0.1287\n",
      "Epoch 2/15\n",
      "35492/35492 [==============================] - 302s 9ms/step - loss: 0.0122 - mae: 0.0748 - val_loss: 0.0338 - val_mae: 0.1201\n",
      "Epoch 3/15\n",
      "35492/35492 [==============================] - 298s 8ms/step - loss: 0.0118 - mae: 0.0730 - val_loss: 0.0315 - val_mae: 0.1154\n",
      "Epoch 4/15\n",
      "35492/35492 [==============================] - 300s 8ms/step - loss: 0.0115 - mae: 0.0720 - val_loss: 0.0300 - val_mae: 0.1134\n",
      "Epoch 5/15\n",
      "35492/35492 [==============================] - 298s 8ms/step - loss: 0.0113 - mae: 0.0713 - val_loss: 0.0262 - val_mae: 0.1062\n",
      "Epoch 6/15\n",
      "35492/35492 [==============================] - 306s 9ms/step - loss: 0.0112 - mae: 0.0708 - val_loss: 0.0237 - val_mae: 0.1016\n",
      "Epoch 7/15\n",
      "35492/35492 [==============================] - 299s 8ms/step - loss: 0.0111 - mae: 0.0704 - val_loss: 0.0219 - val_mae: 0.0979\n",
      "Epoch 8/15\n",
      "35492/35492 [==============================] - 297s 8ms/step - loss: 0.0110 - mae: 0.0701 - val_loss: 0.0205 - val_mae: 0.0947\n",
      "Epoch 9/15\n",
      "35492/35492 [==============================] - 301s 8ms/step - loss: 0.0109 - mae: 0.0699 - val_loss: 0.0195 - val_mae: 0.0929\n",
      "Epoch 10/15\n",
      "35492/35492 [==============================] - 299s 8ms/step - loss: 0.0109 - mae: 0.0696 - val_loss: 0.0188 - val_mae: 0.0911\n",
      "Epoch 11/15\n",
      "35492/35492 [==============================] - 301s 8ms/step - loss: 0.0108 - mae: 0.0694 - val_loss: 0.0185 - val_mae: 0.0908\n",
      "Epoch 12/15\n",
      "35492/35492 [==============================] - 298s 8ms/step - loss: 0.0108 - mae: 0.0692 - val_loss: 0.0176 - val_mae: 0.0884\n",
      "Epoch 13/15\n",
      "35492/35492 [==============================] - 302s 9ms/step - loss: 0.0107 - mae: 0.0691 - val_loss: 0.0172 - val_mae: 0.0877\n",
      "Epoch 14/15\n",
      "35492/35492 [==============================] - 296s 8ms/step - loss: 0.0107 - mae: 0.0689 - val_loss: 0.0171 - val_mae: 0.0877\n",
      "Epoch 15/15\n",
      "35492/35492 [==============================] - 301s 8ms/step - loss: 0.0107 - mae: 0.0689 - val_loss: 0.0166 - val_mae: 0.0860\n",
      "Epoch 1/15\n",
      "35491/35491 [==============================] - 298s 8ms/step - loss: 0.0174 - mae: 0.0919 - val_loss: 0.0489 - val_mae: 0.1440\n",
      "Epoch 2/15\n",
      "35491/35491 [==============================] - 297s 8ms/step - loss: 0.0143 - mae: 0.0815 - val_loss: 0.0414 - val_mae: 0.1329\n",
      "Epoch 3/15\n",
      "35491/35491 [==============================] - 296s 8ms/step - loss: 0.0137 - mae: 0.0795 - val_loss: 0.0347 - val_mae: 0.1234\n",
      "Epoch 4/15\n",
      "35491/35491 [==============================] - 295s 8ms/step - loss: 0.0134 - mae: 0.0783 - val_loss: 0.0298 - val_mae: 0.1150\n",
      "Epoch 5/15\n",
      "35491/35491 [==============================] - 294s 8ms/step - loss: 0.0132 - mae: 0.0776 - val_loss: 0.0272 - val_mae: 0.1103\n",
      "Epoch 6/15\n",
      "35491/35491 [==============================] - 295s 8ms/step - loss: 0.0130 - mae: 0.0770 - val_loss: 0.0256 - val_mae: 0.1075\n",
      "Epoch 7/15\n",
      "35491/35491 [==============================] - 294s 8ms/step - loss: 0.0129 - mae: 0.0767 - val_loss: 0.0239 - val_mae: 0.1040\n",
      "Epoch 8/15\n",
      "35491/35491 [==============================] - 300s 8ms/step - loss: 0.0128 - mae: 0.0763 - val_loss: 0.0235 - val_mae: 0.1032\n",
      "Epoch 9/15\n",
      "35491/35491 [==============================] - 295s 8ms/step - loss: 0.0127 - mae: 0.0761 - val_loss: 0.0226 - val_mae: 0.1013\n",
      "Epoch 10/15\n",
      "35491/35491 [==============================] - 295s 8ms/step - loss: 0.0127 - mae: 0.0758 - val_loss: 0.0222 - val_mae: 0.1006\n",
      "Epoch 11/15\n",
      "35491/35491 [==============================] - 295s 8ms/step - loss: 0.0126 - mae: 0.0756 - val_loss: 0.0219 - val_mae: 0.1003\n",
      "Epoch 12/15\n",
      "35491/35491 [==============================] - 296s 8ms/step - loss: 0.0126 - mae: 0.0755 - val_loss: 0.0213 - val_mae: 0.0986\n",
      "Epoch 13/15\n",
      "35491/35491 [==============================] - 296s 8ms/step - loss: 0.0125 - mae: 0.0753 - val_loss: 0.0210 - val_mae: 0.0984\n",
      "Epoch 14/15\n",
      "35491/35491 [==============================] - 295s 8ms/step - loss: 0.0125 - mae: 0.0752 - val_loss: 0.0208 - val_mae: 0.0978\n",
      "Epoch 15/15\n",
      "35491/35491 [==============================] - 297s 8ms/step - loss: 0.0124 - mae: 0.0751 - val_loss: 0.0205 - val_mae: 0.0971\n"
     ]
    }
   ],
   "source": [
    "last_params = {'width': 0, 'norm': []}\n",
    "for params in parameters:\n",
    "    df = df_initial.copy()\n",
    "    for col in cols:\n",
    "        df[col] = ((params['norm'][1] - params['norm'][0]) * (df[col] - df_initial[col].min()) / (df_initial[col].max() - df_initial[col].min())) + params['norm'][0]\n",
    "\n",
    "    df_train = df[df['date'] < '2019-01-01'].copy()\n",
    "    df_test = df[df['date'] >= '2019-01-01'].copy()\n",
    "\n",
    "    train_X = []\n",
    "    train_Y = []\n",
    "    for i in range(params['width'], len(df_train) - params['output']):\n",
    "        train_X.append(df_train.iloc[i - params['width']:i][input_vars].values)\n",
    "        train_Y.append(df_train.iloc[i:i + params['output']][cols].values)\n",
    "    train_X = np.array(train_X)\n",
    "    train_Y = np.array(train_Y)\n",
    "        \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(params['neurons'], activation='tanh', input_shape=(train_X.shape[1], train_X.shape[2]), return_sequences=False))\n",
    "    model.add(Dropout(params['dropout']))\n",
    "    model.add(Dense(units=len(cols)*params['output'], activation='linear'))\n",
    "    model.add(Reshape((params['output'], len(cols))))\n",
    "    model.compile(optimizer=params['opt'], loss='mse', metrics=['mae'])\n",
    "\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        history = model.fit(train_X, train_Y, epochs=params['epochs'], batch_size=params['batch'], validation_split=0.1, verbose=1, shuffle=False)\n",
    "    model.save(f'models/{station.strip(\".zip\")}_{params[\"width\"]}_{params[\"output\"]}_{params[\"batch\"]}_{params[\"epochs\"]}_{params[\"dropout\"]}_{params[\"neurons\"]}_{params[\"opt\"]}_{params[\"norm\"][0]}{params[\"norm\"][1]}.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Prediction and plotting phase \n",
    "In this phase the predictions of the models trained are done and saved to plot afterwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "directories = os.listdir('models')\n",
    "\n",
    "norm = [model for model in directories if '11' in model]\n",
    "\n",
    "results = []\n",
    "\n",
    "for model_path in norm:\n",
    "    params = model_path.strip('.h5').split('_')\n",
    "    if '01' in params[-1]:\n",
    "        norm_min, norm_max = 0, 1\n",
    "    else:\n",
    "        norm_min, norm_max = -1, 1\n",
    "    df_test = df_initial[df_initial['date'] >= '2019-01-01'].copy()\n",
    "\n",
    "    for col in cols:\n",
    "        df_test[col] = ((norm_max - norm_min) * (df_test[col] - df_initial[col].min()) / (df_initial[col].max() - df_initial[col].min())) + norm_min\n",
    "    \n",
    "    test_X = []\n",
    "    test_Y = []\n",
    "    for i in range(int(params[1]), len(df_test) - int(params[2])):\n",
    "        test_X.append(df_test.iloc[i - int(params[1]):i][input_vars].values)\n",
    "        test_Y.append(df_test.iloc[i:i + int(params[2])][cols].values)\n",
    "    test_X = np.array(test_X)\n",
    "    test_Y = np.array(test_Y)\n",
    "\n",
    "    model = keras.models.load_model(f'models/{model_path}')\n",
    "    y_pred = model.predict(test_X)\n",
    "    for idx, col in enumerate(cols):\n",
    "        y_pred[:, :, idx] = ((y_pred[:, :, idx] - norm_min) * (df_initial[col].max() - df_initial[col].min()) / (norm_max - norm_min)) + df_initial[col].min()\n",
    "    results.append(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for idx, col in enumerate(cols):\n",
    "    test_Y[:, :, idx] = ((test_Y[:, :, idx] - norm_min) * (df_initial[col].max() - df_initial[col].min()) / (norm_max - norm_min)) + df_initial[col].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.96231833,  3.76622882,  0.79760531,  0.81629906,  0.85911466],\n",
       "       [ 2.16530327,  5.11577551,  0.86422847,  1.05259979,  1.13752724],\n",
       "       [ 2.42077212,  6.39498432,  0.94621563,  1.14480225,  1.32011951],\n",
       "       [ 2.70394439,  7.5651705 ,  1.04209604,  1.18927924,  1.46622917],\n",
       "       [ 2.97366116,  8.58757436,  1.14648442,  1.21524349,  1.59543355],\n",
       "       [ 3.20288404,  9.41714545,  1.25850923,  1.23281881,  1.70628779],\n",
       "       [ 3.39241135, 10.04063588,  1.37167028,  1.24570001,  1.79890891],\n",
       "       [ 3.54180623, 10.48484906,  1.48280428,  1.25998519,  1.87729822],\n",
       "       [ 3.66660678, 10.81775076,  1.59024521,  1.27352715,  1.94790947],\n",
       "       [ 3.78669148, 11.10232937,  1.6944867 ,  1.28732667,  2.00477584],\n",
       "       [ 3.93250345, 11.39853821,  1.79685162,  1.30145045,  2.05761646],\n",
       "       [ 4.09961292, 11.71042747,  1.8976519 ,  1.31444153,  2.10692212],\n",
       "       [ 4.28207862, 12.04308611,  1.99608317,  1.32690872,  2.14953698],\n",
       "       [ 4.48136405, 12.41071781,  2.09364728,  1.33595206,  2.18676263],\n",
       "       [ 4.70586906, 12.83978997,  2.19162697,  1.3425472 ,  2.22161707],\n",
       "       [ 4.96047907, 13.35701705,  2.28990157,  1.35138736,  2.25665187]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute the rmse\n",
    "rmse = np.sqrt(np.mean(((y_pred - test_Y) ** 2), axis=0))\n",
    "rmse "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum = np.inf\n",
    "for idx, result in enumerate(results):\n",
    "    if len(result) < minimum:\n",
    "        minimum = len(result)\n",
    "for idx, result in enumerate(results):\n",
    "    results[idx] = result[-minimum:]\n",
    "test_Y = test_Y[-minimum:, :, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = [results[0][:, idx, :] for idx in range(results[0].shape[1])]\n",
    "test = [test_Y[:, idx, :] for idx in range(test_Y.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17480, 16, 5)\n"
     ]
    }
   ],
   "source": [
    "print(results[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for idx, col in enumerate(cols):\n",
    "    # plot each column\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(test[0][:480, idx], label='Real', color='blue')\n",
    "    for idx2, result in enumerate(results):\n",
    "        for i in range(0, result.shape[1], 1):\n",
    "            params = norm[idx2].strip('.h5').split('_')\n",
    "            rmse = np.sqrt(np.mean(np.power((test[0][:, idx] - result[:, i, idx]), 2)))\n",
    "            plt.plot(result[:480, i, idx], label=f'{i*30} Minutes: {rmse:.4f}')\n",
    "    plt.legend()\n",
    "    plt.title(f'{col} - Batch size - {params[-1]} Normalization')\n",
    "    plt.savefig(f'plots/{col}_batchSize_{params[-1]}norm.png')\n",
    "    plt.clf()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Treated outliers\n",
    "\n",
    "Modelos entrenados usando IQR como tratamiento de outliers, hay dos casos:\n",
    "- Datos normalizados entre 0 y 1\n",
    "- Datos normalizados entre -1 y 1\n",
    "\n",
    "Los modelos en la carpeta models se identifican de la siguiente forma:\n",
    "\n",
    "{estación}\\_{ventana temporal}\\_{numero de outputs}\\_{batch size}\\_{epochs}\\_{drop out}\\_{neuronas}\\_{optimizer}\\_{normalization}.h5\n",
    "\n",
    "Por ejemplo: **C6_24_1_6_20_0.05_64_adam_o-11.h5**\n",
    "- Datos de la estación C6\n",
    "- La ventana temporal es de 24 (24 datos previos al instante predicho, cada una de estas 24 representa una medición cada 30min)\n",
    "- Solo hay una salida (5 valores pero solo 30 minutos)\n",
    "- Batch size de 6.\n",
    "- Entrenado durante 20 epochs\n",
    "- Drop out de 0.05\n",
    "- La capa LSTM tiene 64 neuronas.\n",
    "- El optimizador es ADAM\n",
    "- Los datos son normalizados entre -1 y 1 y con los valores extremos tratados usando IQR. \n",
    "\n",
    "\n",
    "\n",
    "## 2.1. Training phase\n",
    "In here the training parameters for the sesion are decided. A list of dicts that contains the parameters of each model is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_width = [24]\n",
    "prediction_width = [1]\n",
    "batch_size = [6]\n",
    "epochs = [15]\n",
    "dropout = [0.05]\n",
    "neurons = [64]\n",
    "optimizer = ['adam']\n",
    "normalization = [[-1, 1]]#, [0, 1]]\n",
    "\n",
    "station = 'C6.zip'\n",
    "variables = ['T', 'HR', 'P', 'u10', 'v10', 'day', 'time', 'date']\n",
    "input_vars = ['T', 'HR', 'P', 'u10', 'v10', 'day', 'time']\n",
    "cols = ['T', 'HR', 'P', 'u10', 'v10']\n",
    "\n",
    "df_initial = pd.read_csv(f'data/data_by_station/{station}', compression='zip', header=0, sep=',')\n",
    "df_initial['date'] = pd.to_datetime(df_initial['date'], format='%Y-%m-%d %H:%M:%S')\n",
    "df_initial['day'] = df_initial['date'].dt.dayofyear / 365\n",
    "df_initial['time'] = df_initial['date'].dt.hour / 24\n",
    "df_initial = df_initial.astype({'T': 'float', 'HR': 'float', 'P': 'float', 'u2': 'float', 'v2': 'float', 'u6': 'float', 'v6': 'float', 'u10': 'float', 'v10': 'float', 'altitud': 'float', 'latitud': 'float', 'longitud': 'float'})\n",
    "df_initial = df_initial[variables]\n",
    "\n",
    "parameters = []\n",
    "for i in input_width:\n",
    "    for j in prediction_width:\n",
    "        for k in batch_size:\n",
    "            for l in epochs:\n",
    "                for m in dropout:\n",
    "                    for n in neurons:\n",
    "                            for p in optimizer:\n",
    "                                for s in normalization:\n",
    "                                    parameters.append({'width':i, 'output': j, 'batch': k, 'epochs': l, 'dropout': m, 'neurons': n, 'opt': p, 'norm': s})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "17747/17747 [==============================] - 151s 8ms/step - loss: 0.0248 - mae: 0.1027 - val_loss: 0.0258 - val_mae: 0.1075\n",
      "Epoch 2/15\n",
      "17747/17747 [==============================] - 146s 8ms/step - loss: 0.0227 - mae: 0.0967 - val_loss: 0.0274 - val_mae: 0.1111\n",
      "Epoch 3/15\n",
      "17747/17747 [==============================] - 148s 8ms/step - loss: 0.0220 - mae: 0.0951 - val_loss: 0.0254 - val_mae: 0.1060\n",
      "Epoch 4/15\n",
      "17747/17747 [==============================] - 147s 8ms/step - loss: 0.0216 - mae: 0.0939 - val_loss: 0.0247 - val_mae: 0.1036\n",
      "Epoch 5/15\n",
      "17747/17747 [==============================] - 148s 8ms/step - loss: 0.0212 - mae: 0.0930 - val_loss: 0.0241 - val_mae: 0.1023\n",
      "Epoch 6/15\n",
      "17747/17747 [==============================] - 148s 8ms/step - loss: 0.0209 - mae: 0.0923 - val_loss: 0.0236 - val_mae: 0.1007\n",
      "Epoch 7/15\n",
      "17747/17747 [==============================] - 149s 8ms/step - loss: 0.0207 - mae: 0.0916 - val_loss: 0.0232 - val_mae: 0.0990\n",
      "Epoch 8/15\n",
      "17747/17747 [==============================] - 148s 8ms/step - loss: 0.0204 - mae: 0.0910 - val_loss: 0.0231 - val_mae: 0.0995\n",
      "Epoch 9/15\n",
      "17747/17747 [==============================] - 147s 8ms/step - loss: 0.0202 - mae: 0.0904 - val_loss: 0.0233 - val_mae: 0.1000\n",
      "Epoch 10/15\n",
      "17747/17747 [==============================] - 148s 8ms/step - loss: 0.0200 - mae: 0.0900 - val_loss: 0.0236 - val_mae: 0.1002\n",
      "Epoch 11/15\n",
      "17747/17747 [==============================] - 148s 8ms/step - loss: 0.0199 - mae: 0.0896 - val_loss: 0.0242 - val_mae: 0.1019\n",
      "Epoch 12/15\n",
      "17747/17747 [==============================] - 147s 8ms/step - loss: 0.0197 - mae: 0.0893 - val_loss: 0.0237 - val_mae: 0.1001\n",
      "Epoch 13/15\n",
      "17747/17747 [==============================] - 149s 8ms/step - loss: 0.0196 - mae: 0.0890 - val_loss: 0.0238 - val_mae: 0.1002\n",
      "Epoch 14/15\n",
      "17747/17747 [==============================] - 148s 8ms/step - loss: 0.0196 - mae: 0.0888 - val_loss: 0.0238 - val_mae: 0.1003\n",
      "Epoch 15/15\n",
      "17747/17747 [==============================] - 148s 8ms/step - loss: 0.0195 - mae: 0.0886 - val_loss: 0.0235 - val_mae: 0.0999\n"
     ]
    }
   ],
   "source": [
    "last_params = {'width': 0, 'norm': []}\n",
    "min_maxs = {}\n",
    "\n",
    "for params in parameters:\n",
    "    df = df_initial.copy()\n",
    "    for col in cols:\n",
    "        iqr = df_initial[col].quantile(0.75) - df_initial[col].quantile(0.25)\n",
    "        min_maxs[col] = [df_initial[col].quantile(0.25) - 1.5 * iqr, df_initial[col].quantile(0.75) + 1.5 * iqr]\n",
    "        df[col] = ((params['norm'][1] - params['norm'][0]) * (df[col] - min_maxs[col][0]) / (min_maxs[col][1] - min_maxs[col][0])) + params['norm'][0]\n",
    "        df.loc[df[col] < params['norm'][0], col] = params['norm'][0]\n",
    "        df.loc[df[col] > params['norm'][1], col] = params['norm'][1]\n",
    "        \n",
    "    df_train = df[df['date'] < '2019-01-01'].copy()\n",
    "    df_test = df[df['date'] >= '2019-01-01'].copy()\n",
    "\n",
    "    train_X = []\n",
    "    train_Y = []\n",
    "    for i in range(params['width'], len(df_train) - params['output']):\n",
    "        train_X.append(df_train.iloc[i - params['width']:i][input_vars].values)\n",
    "        train_Y.append(df_train.iloc[i:i + params['output']][cols].values)\n",
    "    train_X = np.array(train_X)\n",
    "    train_Y = np.array(train_Y)\n",
    "\n",
    "    test_X = []\n",
    "    test_Y = []\n",
    "    for i in range(params['width'], len(df_test) - params['output']):\n",
    "        test_X.append(df_test.iloc[i - params['width']:i][input_vars].values)\n",
    "        test_Y.append(df_test.iloc[i:i + params['output']][cols].values)\n",
    "    test_X = np.array(test_X)\n",
    "    test_Y = np.array(test_Y)\n",
    "        \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(params['neurons'], activation='tanh', input_shape=(train_X.shape[1], train_X.shape[2]), return_sequences=False))\n",
    "    model.add(Dropout(params['dropout']))\n",
    "    model.add(Dense(units=len(cols), activation='linear'))\n",
    "    model.compile(optimizer=params['opt'], loss='mse', metrics=['mae'])\n",
    "\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        history = model.fit(train_X, train_Y, epochs=params['epochs'], batch_size=params['batch'], validation_split=0.1, verbose=1, shuffle=False)\n",
    "    model.save(f'models/{station.strip(\".zip\")}_{params[\"width\"]}_{params[\"output\"]}_{params[\"batch\"]}_{params[\"epochs\"]}_{params[\"dropout\"]}_{params[\"neurons\"]}_{params[\"opt\"]}_{params[\"norm\"][0]}{params[\"norm\"][1]}.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Prediction and plotting phase \n",
    "In this phase the predictions of the models trained are done and saved to plot afterwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "547/547 [==============================] - 2s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "directories = os.listdir('models')\n",
    "\n",
    "norm = [model for model in directories if '11' in model]\n",
    "\n",
    "results = []\n",
    "\n",
    "for model_path in norm:\n",
    "    params = model_path.strip('.h5').split('_')\n",
    "    if '01' in params[-1]:\n",
    "        norm_min, norm_max = 0, 1\n",
    "    else:\n",
    "        norm_min, norm_max = -1, 1\n",
    "    df_test = df_initial[df_initial['date'] >= '2019-01-01'].copy()\n",
    "\n",
    "    for col in cols:\n",
    "        iqr = df_initial[col].quantile(0.75) - df_initial[col].quantile(0.25)\n",
    "        min_maxs[col] = [df_initial[col].quantile(0.25) - 1.5 * iqr, df_initial[col].quantile(0.75) + 1.5 * iqr]\n",
    "        df_test[col] = ((norm_max - norm_min) * (df_test[col] - min_maxs[col][0]) / (min_maxs[col][1] - min_maxs[col][0])) + norm_min\n",
    "        df_test.loc[df_test[col] < norm_min, col] = norm_min\n",
    "        df_test.loc[df_test[col] > norm_max, col] = norm_max\n",
    "    \n",
    "    test_X = []\n",
    "    test_Y = []\n",
    "    for i in range(int(params[1]), len(df_test) - int(params[2])):\n",
    "        test_X.append(df_test.iloc[i - int(params[1]):i][input_vars].values)\n",
    "        test_Y.append(df_test.iloc[i:i + int(params[2])][cols].values)\n",
    "    test_X = np.array(test_X)\n",
    "    test_Y = np.array(test_Y)\n",
    "\n",
    "    model = keras.models.load_model(f'models/{model_path}')\n",
    "    y_pred = model.predict(test_X)\n",
    "    for idx, col in enumerate(cols):\n",
    "        y_pred[:, idx] = ((y_pred[:, idx] - norm_min) * (df_initial[col].max() - df_initial[col].min()) / (norm_max - norm_min)) + df_initial[col].min()\n",
    "    \n",
    "    results.append(y_pred)\n",
    "\n",
    "for idx, col in enumerate(cols):\n",
    "    test_Y[:, 0, idx] = ((test_Y[:, 0, idx] - norm_min) * (df_initial[col].max() - df_initial[col].min()) / (norm_max - norm_min)) + df_initial[col].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "minimum = np.inf\n",
    "for idx, result in enumerate(results):\n",
    "    if len(result) < minimum:\n",
    "        minimum = len(result)\n",
    "for idx, result in enumerate(results):\n",
    "    results[idx] = result[-minimum:]\n",
    "test_Y = test_Y[-minimum:, 0, :]\n",
    "\n",
    "for idx, col in enumerate(cols):\n",
    "    # plot each column\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(test_Y[:, idx], label='Real', color='blue')\n",
    "    for idx2, result in enumerate(results):\n",
    "        params = norm[idx2].strip('.h5').split('_')\n",
    "        rmse = np.sqrt(np.mean(np.power((test_Y[:, idx] - result[:, idx]), 2)))\n",
    "        plt.plot(result[:, idx], label=f'RMSE {rmse:.4f}', color='g')\n",
    "    plt.legend()\n",
    "    plt.title(f'{col} - Drop out - {params[-1]} Normalization (No outliers)')\n",
    "    plt.savefig(f'plots/{col}_dropOut_o{params[-1]}norm.png')\n",
    "    plt.clf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
