{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", tf.config.list_physical_devices())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Horizonte N normal\n",
    "Modelos entrenados sin tratamiento de outliers, hay dos casos:\n",
    "- Datos normalizados entre 0 y 1\n",
    "- Datos normalizados entre -1 y 1\n",
    "\n",
    "Los modelos en la carpeta models se identifican de la siguiente forma:\n",
    "\n",
    "{estación}\\_{ventana temporal}\\_{numero de outputs}\\_{batch size}\\_{epochs}\\_{drop out}\\_{neuronas}\\_{optimizer}\\_{normalization}.h5\n",
    "\n",
    "Por ejemplo: **C6_24_1_6_20_0.05_64_adam_-11.h5**\n",
    "- Datos de la estación C6\n",
    "- La ventana temporal es de 24 (24 datos previos al instante predicho, cada una de estas 24 representa una medición cada 30min)\n",
    "- Solo hay una salida (5 valores pero solo 30 minutos)\n",
    "- Batch size de 6.\n",
    "- Entrenado durante 20 epochs\n",
    "- Drop out de 0.05\n",
    "- La capa LSTM tiene 64 neuronas.\n",
    "- El optimizador es ADAM\n",
    "- Los datos son normalizados entre -1 y 1. \n",
    "\n",
    "\n",
    "\n",
    "## 1.1. Training phase\n",
    "In here the training parameters for the sesion are decided. A list of dicts that contains the parameters of each model is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_width = [48]\n",
    "prediction_width = [48]\n",
    "batch_size = [6]\n",
    "epochs = [10]\n",
    "dropout = [0.05]\n",
    "neurons = [64]\n",
    "optimizer = ['adagrad']\n",
    "normalization = [[-1, 1]]#, [0, 1]]\n",
    "\n",
    "station = 'D6.zip'\n",
    "variables = ['T', 'HR', 'P', 'u10', 'v10', 'day', 'time', 'date']\n",
    "input_vars = ['T', 'HR', 'P', 'u10', 'v10', 'day', 'time']\n",
    "cols = ['T', 'HR', 'P', 'u10', 'v10']\n",
    "\n",
    "df_initial = pd.read_csv(f'data/data_by_station/{station}', compression='zip', header=0, sep=',')\n",
    "df_initial['date'] = pd.to_datetime(df_initial['date'], format='%Y-%m-%d %H:%M:%S')\n",
    "df_initial['day'] = df_initial['date'].dt.dayofyear / 365\n",
    "df_initial['time'] = df_initial['date'].dt.hour / 24\n",
    "df_initial = df_initial.astype({'T': 'float', 'HR': 'float', 'P': 'float', 'u2': 'float', 'v2': 'float', 'u6': 'float', 'v6': 'float', 'u10': 'float', 'v10': 'float', 'altitud': 'float', 'latitud': 'float', 'longitud': 'float'})\n",
    "df_initial = df_initial[variables]\n",
    "\n",
    "parameters = []\n",
    "for i in input_width:\n",
    "    for j in prediction_width:\n",
    "        for k in batch_size:\n",
    "            for l in epochs:\n",
    "                for m in dropout:\n",
    "                    for n in neurons:\n",
    "                            for p in optimizer:\n",
    "                                for s in normalization:\n",
    "                                    parameters.append({'width':i, 'output': j, 'batch': k, 'epochs': l, 'dropout': m, 'neurons': n, 'opt': p, 'norm': s})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "23649/23649 [==============================] - 204s 9ms/step - loss: 0.1016 - mae: 0.2373 - val_loss: 0.0621 - val_mae: 0.1801\n",
      "Epoch 2/10\n",
      "23649/23649 [==============================] - 200s 8ms/step - loss: 0.0627 - mae: 0.1847 - val_loss: 0.0576 - val_mae: 0.1765\n",
      "Epoch 3/10\n",
      "23649/23649 [==============================] - 202s 9ms/step - loss: 0.0554 - mae: 0.1751 - val_loss: 0.0475 - val_mae: 0.1630\n",
      "Epoch 4/10\n",
      "23649/23649 [==============================] - 201s 9ms/step - loss: 0.0466 - mae: 0.1600 - val_loss: 0.0398 - val_mae: 0.1469\n",
      "Epoch 5/10\n",
      "23649/23649 [==============================] - 204s 9ms/step - loss: 0.0410 - mae: 0.1482 - val_loss: 0.0359 - val_mae: 0.1369\n",
      "Epoch 6/10\n",
      "23649/23649 [==============================] - 203s 9ms/step - loss: 0.0382 - mae: 0.1417 - val_loss: 0.0341 - val_mae: 0.1317\n",
      "Epoch 7/10\n",
      "23649/23649 [==============================] - 204s 9ms/step - loss: 0.0368 - mae: 0.1382 - val_loss: 0.0332 - val_mae: 0.1288\n",
      "Epoch 8/10\n",
      "23649/23649 [==============================] - 204s 9ms/step - loss: 0.0360 - mae: 0.1361 - val_loss: 0.0326 - val_mae: 0.1271\n",
      "Epoch 9/10\n",
      "23649/23649 [==============================] - 204s 9ms/step - loss: 0.0354 - mae: 0.1347 - val_loss: 0.0322 - val_mae: 0.1259\n",
      "Epoch 10/10\n",
      "23649/23649 [==============================] - 205s 9ms/step - loss: 0.0350 - mae: 0.1336 - val_loss: 0.0318 - val_mae: 0.1250\n"
     ]
    }
   ],
   "source": [
    "last_params = {'width': 0, 'norm': []}\n",
    "for params in parameters:\n",
    "    df = df_initial.copy()\n",
    "    for col in cols:\n",
    "        df[col] = ((params['norm'][1] - params['norm'][0]) * (df[col] - df_initial[col].min()) / (df_initial[col].max() - df_initial[col].min())) + params['norm'][0]\n",
    "\n",
    "    df_train = df[df['date'] < '2019-01-01'].copy()\n",
    "    df_test = df[df['date'] >= '2019-01-01'].copy()\n",
    "\n",
    "    train_X = []\n",
    "    train_Y = []\n",
    "    for i in range(params['width'], len(df_train) - params['output']):\n",
    "        train_X.append(df_train.iloc[i - params['width']:i][input_vars].values)\n",
    "        train_Y.append(df_train.iloc[i:i + params['output']][cols].values)\n",
    "    train_X = np.array(train_X)\n",
    "    train_Y = np.array(train_Y)\n",
    "        \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(params['neurons'], activation='tanh', input_shape=(train_X.shape[1], train_X.shape[2]), return_sequences=False))\n",
    "    model.add(Dropout(params['dropout']))\n",
    "    model.add(Dense(units=len(cols)*params['output'], activation='linear'))\n",
    "    model.add(Reshape((params['output'], len(cols))))\n",
    "    model.compile(optimizer=params['opt'], loss='mse', metrics=['mae'])\n",
    "\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        history = model.fit(train_X, train_Y, epochs=params['epochs'], batch_size=params['batch'], validation_split=0.1, verbose=1, shuffle=False)\n",
    "    model.save(f'models/{station.strip(\".zip\")}_{params[\"width\"]}_{params[\"output\"]}_{params[\"batch\"]}_{params[\"epochs\"]}_{params[\"dropout\"]}_{params[\"neurons\"]}_{params[\"opt\"]}_{params[\"norm\"][0]}{params[\"norm\"][1]}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_width = [48]\n",
    "prediction_width = [48]\n",
    "batch_size = [6]\n",
    "epochs = [10]\n",
    "dropout = [0.05]\n",
    "neurons = [64]\n",
    "optimizer = ['adagrad']\n",
    "normalization = [[-1, 1]]#, [0, 1]]\n",
    "\n",
    "station = 'CL.zip'\n",
    "variables = ['T', 'HR', 'P', 'u10', 'v10', 'day', 'time', 'date']\n",
    "input_vars = ['T', 'HR', 'P', 'u10', 'v10', 'day', 'time']\n",
    "cols = ['T', 'HR', 'P', 'u10', 'v10']\n",
    "\n",
    "df_initial = pd.read_csv(f'data/data_by_station/{station}', compression='zip', header=0, sep=',')\n",
    "df_initial['date'] = pd.to_datetime(df_initial['date'], format='%Y-%m-%d %H:%M:%S')\n",
    "df_initial['day'] = df_initial['date'].dt.dayofyear / 365\n",
    "df_initial['time'] = df_initial['date'].dt.hour / 24\n",
    "df_initial = df_initial.astype({'T': 'float', 'HR': 'float', 'P': 'float', 'u2': 'float', 'v2': 'float', 'u6': 'float', 'v6': 'float', 'u10': 'float', 'v10': 'float', 'altitud': 'float', 'latitud': 'float', 'longitud': 'float'})\n",
    "df_initial = df_initial[variables]\n",
    "\n",
    "parameters = []\n",
    "for i in input_width:\n",
    "    for j in prediction_width:\n",
    "        for k in batch_size:\n",
    "            for l in epochs:\n",
    "                for m in dropout:\n",
    "                    for n in neurons:\n",
    "                            for p in optimizer:\n",
    "                                for s in normalization:\n",
    "                                    parameters.append({'width':i, 'output': j, 'batch': k, 'epochs': l, 'dropout': m, 'neurons': n, 'opt': p, 'norm': s})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "23647/23647 [==============================] - 199s 8ms/step - loss: 0.1033 - mae: 0.2425 - val_loss: 0.0761 - val_mae: 0.2008\n",
      "Epoch 2/10\n",
      "23647/23647 [==============================] - 190s 8ms/step - loss: 0.0749 - mae: 0.2009 - val_loss: 0.0670 - val_mae: 0.1884\n",
      "Epoch 3/10\n",
      "23647/23647 [==============================] - 193s 8ms/step - loss: 0.0667 - mae: 0.1894 - val_loss: 0.0589 - val_mae: 0.1764\n",
      "Epoch 4/10\n",
      "23647/23647 [==============================] - 193s 8ms/step - loss: 0.0588 - mae: 0.1774 - val_loss: 0.0506 - val_mae: 0.1642\n",
      "Epoch 5/10\n",
      "23647/23647 [==============================] - 190s 8ms/step - loss: 0.0500 - mae: 0.1647 - val_loss: 0.0415 - val_mae: 0.1498\n",
      "Epoch 6/10\n",
      "23647/23647 [==============================] - 193s 8ms/step - loss: 0.0421 - mae: 0.1508 - val_loss: 0.0355 - val_mae: 0.1367\n",
      "Epoch 7/10\n",
      "23647/23647 [==============================] - 198s 8ms/step - loss: 0.0376 - mae: 0.1410 - val_loss: 0.0324 - val_mae: 0.1287\n",
      "Epoch 8/10\n",
      "23647/23647 [==============================] - 195s 8ms/step - loss: 0.0351 - mae: 0.1350 - val_loss: 0.0304 - val_mae: 0.1235\n",
      "Epoch 9/10\n",
      "23647/23647 [==============================] - 190s 8ms/step - loss: 0.0335 - mae: 0.1310 - val_loss: 0.0289 - val_mae: 0.1196\n",
      "Epoch 10/10\n",
      "23647/23647 [==============================] - 191s 8ms/step - loss: 0.0323 - mae: 0.1277 - val_loss: 0.0277 - val_mae: 0.1164\n"
     ]
    }
   ],
   "source": [
    "last_params = {'width': 0, 'norm': []}\n",
    "for params in parameters:\n",
    "    df = df_initial.copy()\n",
    "    for col in cols:\n",
    "        df[col] = ((params['norm'][1] - params['norm'][0]) * (df[col] - df_initial[col].min()) / (df_initial[col].max() - df_initial[col].min())) + params['norm'][0]\n",
    "\n",
    "    df_train = df[df['date'] < '2019-01-01'].copy()\n",
    "    df_test = df[df['date'] >= '2019-01-01'].copy()\n",
    "\n",
    "    train_X = []\n",
    "    train_Y = []\n",
    "    for i in range(params['width'], len(df_train) - params['output']):\n",
    "        train_X.append(df_train.iloc[i - params['width']:i][input_vars].values)\n",
    "        train_Y.append(df_train.iloc[i:i + params['output']][cols].values)\n",
    "    train_X = np.array(train_X)\n",
    "    train_Y = np.array(train_Y)\n",
    "        \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(params['neurons'], activation='tanh', input_shape=(train_X.shape[1], train_X.shape[2]), return_sequences=False))\n",
    "    model.add(Dropout(params['dropout']))\n",
    "    model.add(Dense(units=len(cols)*params['output'], activation='linear'))\n",
    "    model.add(Reshape((params['output'], len(cols))))\n",
    "    model.compile(optimizer=params['opt'], loss='mse', metrics=['mae'])\n",
    "\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        history = model.fit(train_X, train_Y, epochs=params['epochs'], batch_size=params['batch'], validation_split=0.1, verbose=1, shuffle=False)\n",
    "    model.save(f'models/{station.strip(\".zip\")}_{params[\"width\"]}_{params[\"output\"]}_{params[\"batch\"]}_{params[\"epochs\"]}_{params[\"dropout\"]}_{params[\"neurons\"]}_{params[\"opt\"]}_{params[\"norm\"][0]}{params[\"norm\"][1]}.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Prediction and plotting phase \n",
    "In this phase the predictions of the models trained are done and saved to plot afterwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"d:\\MARC\\UNI\\MASTER\\TFM\\Wind-Fire_prediction\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1845, in predict_function  *\n        return step_function(self, iterator)\n    File \"d:\\MARC\\UNI\\MASTER\\TFM\\Wind-Fire_prediction\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1834, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"d:\\MARC\\UNI\\MASTER\\TFM\\Wind-Fire_prediction\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1823, in run_step  **\n        outputs = model.predict_step(data)\n    File \"d:\\MARC\\UNI\\MASTER\\TFM\\Wind-Fire_prediction\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1791, in predict_step\n        return self(x, training=False)\n    File \"d:\\MARC\\UNI\\MASTER\\TFM\\Wind-Fire_prediction\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"d:\\MARC\\UNI\\MASTER\\TFM\\Wind-Fire_prediction\\venv\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 264, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"sequential_2\" is incompatible with the layer: expected shape=(None, 48, 6), found shape=(None, 48, 7)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1712\\1786332131.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'models/{model_path}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0my_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mnorm_min\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdf_initial\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mdf_initial\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnorm_max\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mnorm_min\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdf_initial\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\MARC\\UNI\\MASTER\\TFM\\Wind-Fire_prediction\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\MARC\\UNI\\MASTER\\TFM\\Wind-Fire_prediction\\venv\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtf__predict_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                     \u001b[0mretval_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m                 \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"d:\\MARC\\UNI\\MASTER\\TFM\\Wind-Fire_prediction\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1845, in predict_function  *\n        return step_function(self, iterator)\n    File \"d:\\MARC\\UNI\\MASTER\\TFM\\Wind-Fire_prediction\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1834, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"d:\\MARC\\UNI\\MASTER\\TFM\\Wind-Fire_prediction\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1823, in run_step  **\n        outputs = model.predict_step(data)\n    File \"d:\\MARC\\UNI\\MASTER\\TFM\\Wind-Fire_prediction\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1791, in predict_step\n        return self(x, training=False)\n    File \"d:\\MARC\\UNI\\MASTER\\TFM\\Wind-Fire_prediction\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"d:\\MARC\\UNI\\MASTER\\TFM\\Wind-Fire_prediction\\venv\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 264, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"sequential_2\" is incompatible with the layer: expected shape=(None, 48, 6), found shape=(None, 48, 7)\n"
     ]
    }
   ],
   "source": [
    "directories = os.listdir('models')\n",
    "\n",
    "norm = [model for model in directories if '11' in model]\n",
    "\n",
    "results = []\n",
    "\n",
    "for model_path in norm:\n",
    "    params = model_path.strip('.h5').split('_')\n",
    "    if '01' in params[-1]:\n",
    "        norm_min, norm_max = 0, 1\n",
    "    else:\n",
    "        norm_min, norm_max = -1, 1\n",
    "    df_test = df_initial[df_initial['date'] >= '2019-01-01'].copy()\n",
    "\n",
    "    for col in cols:\n",
    "        df_test[col] = ((norm_max - norm_min) * (df_test[col] - df_initial[col].min()) / (df_initial[col].max() - df_initial[col].min())) + norm_min\n",
    "    \n",
    "    test_X = []\n",
    "    test_Y = []\n",
    "    for i in range(int(params[1]), len(df_test) - int(params[2])):\n",
    "        test_X.append(df_test.iloc[i - int(params[1]):i][input_vars].values)\n",
    "        test_Y.append(df_test.iloc[i:i + int(params[2])][cols].values)\n",
    "    test_X = np.array(test_X)\n",
    "    test_Y = np.array(test_Y)\n",
    "\n",
    "    model = keras.models.load_model(f'models/{model_path}')\n",
    "    y_pred = model.predict(test_X)\n",
    "    for idx, col in enumerate(cols):\n",
    "        y_pred[:, :, idx] = ((y_pred[:, :, idx] - norm_min) * (df_initial[col].max() - df_initial[col].min()) / (norm_max - norm_min)) + df_initial[col].min()\n",
    "    results.append(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, col in enumerate(cols):\n",
    "    test_Y[:, :, idx] = ((test_Y[:, :, idx] - norm_min) * (df_initial[col].max() - df_initial[col].min()) / (norm_max - norm_min)) + df_initial[col].min()\n",
    "    \n",
    "rmse = np.sqrt(np.mean(((y_pred - test_Y) ** 2), axis=0))\n",
    "test = [test_Y[:, idx, :] for idx in range(test_Y.shape[1])]\n",
    "\n",
    "s1 = results[0].shape[1]\n",
    "for idx, col in enumerate(cols):\n",
    "    # plot each column\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(test[0][s1:480+s1, idx], label='Real', color='blue')\n",
    "    for idx2, result in enumerate(results):\n",
    "        for i in range(1, result.shape[1], 2):\n",
    "            params = norm[idx2].strip('.h5').split('_')\n",
    "            rmse = np.sqrt(np.mean(np.power((test[0][:, idx] - result[:, i, idx]), 2)))\n",
    "            plt.plot(result[s1-i:480+(s1-i), i, idx], label=f'{i*30} Minutes: {rmse:.4f}')\n",
    "    plt.legend()\n",
    "    plt.title(f'{col} - Batch size - {params[-1]} Normalization')\n",
    "    plt.savefig(f'plots/{col}_batchSize_{params[-1]}norm.png')\n",
    "    plt.clf()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Horizonte N \"recursivo\"\n",
    "\n",
    "Modelos entrenados usando IQR como tratamiento de outliers, hay dos casos:\n",
    "- Datos normalizados entre 0 y 1\n",
    "- Datos normalizados entre -1 y 1\n",
    "\n",
    "Los modelos en la carpeta models se identifican de la siguiente forma:\n",
    "\n",
    "{estación}\\_{ventana temporal}\\_{numero de outputs}\\_{batch size}\\_{epochs}\\_{drop out}\\_{neuronas}\\_{optimizer}\\_{normalization}.h5\n",
    "\n",
    "Por ejemplo: **C6_24_1_6_20_0.05_64_adam_o-11.h5**\n",
    "- Datos de la estación C6\n",
    "- La ventana temporal es de 24 (24 datos previos al instante predicho, cada una de estas 24 representa una medición cada 30min)\n",
    "- Solo hay una salida (5 valores pero solo 30 minutos)\n",
    "- Batch size de 6.\n",
    "- Entrenado durante 20 epochs\n",
    "- Drop out de 0.05\n",
    "- La capa LSTM tiene 64 neuronas.\n",
    "- El optimizador es ADAM\n",
    "- Los datos son normalizados entre -1 y 1 y con los valores extremos tratados usando IQR. \n",
    "\n",
    "\n",
    "\n",
    "## 2.1. Training phase\n",
    "In here the training parameters for the sesion are decided. A list of dicts that contains the parameters of each model is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_width = [24]\n",
    "prediction_width = [1]\n",
    "batch_size = [6]\n",
    "epochs = [15]\n",
    "dropout = [0.05]\n",
    "neurons = [64]\n",
    "optimizer = ['adam']\n",
    "normalization = [[-1, 1]]#, [0, 1]]\n",
    "\n",
    "station = 'D6.zip'\n",
    "variables = ['T', 'HR', 'P', 'u10', 'v10', 'day', 'time', 'date']\n",
    "input_vars = ['T', 'HR', 'P', 'u10', 'v10', 'day', 'time']\n",
    "cols = ['T', 'HR', 'P', 'u10', 'v10']\n",
    "\n",
    "df_initial = pd.read_csv(f'data/data_by_station/{station}', compression='zip', header=0, sep=',')\n",
    "df_initial['date'] = pd.to_datetime(df_initial['date'], format='%Y-%m-%d %H:%M:%S')\n",
    "df_initial['day'] = df_initial['date'].dt.dayofyear / 365\n",
    "df_initial['time'] = df_initial['date'].dt.hour / 24\n",
    "df_initial = df_initial.astype({'T': 'float', 'HR': 'float', 'P': 'float', 'u2': 'float', 'v2': 'float', 'u6': 'float', 'v6': 'float', 'u10': 'float', 'v10': 'float', 'altitud': 'float', 'latitud': 'float', 'longitud': 'float'})\n",
    "df_initial = df_initial[variables]\n",
    "\n",
    "parameters = []\n",
    "for i in input_width:\n",
    "    for j in prediction_width:\n",
    "        for k in batch_size:\n",
    "            for l in epochs:\n",
    "                for m in dropout:\n",
    "                    for n in neurons:\n",
    "                            for p in optimizer:\n",
    "                                for s in normalization:\n",
    "                                    parameters.append({'width':i, 'output': j, 'batch': k, 'epochs': l, 'dropout': m, 'neurons': n, 'opt': p, 'norm': s})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Prediction and plotting phase \n",
    "In this phase the predictions of the models trained are done and saved to plot afterwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "546/546 [==============================] - 10s 4ms/step\n",
      "546/546 [==============================] - 2s 4ms/step\n",
      "546/546 [==============================] - 3s 5ms/step\n",
      "546/546 [==============================] - 2s 4ms/step\n",
      "546/546 [==============================] - 2s 4ms/step\n",
      "546/546 [==============================] - 2s 4ms/step\n",
      "546/546 [==============================] - 2s 4ms/step\n",
      "546/546 [==============================] - 2s 4ms/step\n",
      "546/546 [==============================] - 2s 4ms/step\n",
      "546/546 [==============================] - 2s 4ms/step\n",
      "546/546 [==============================] - 2s 5ms/step\n",
      "546/546 [==============================] - 2s 4ms/step\n",
      "546/546 [==============================] - 2s 4ms/step\n",
      "546/546 [==============================] - 2s 4ms/step\n",
      "546/546 [==============================] - 2s 4ms/step\n",
      "546/546 [==============================] - 2s 4ms/step\n",
      "546/546 [==============================] - 2s 4ms/step\n",
      "546/546 [==============================] - 2s 4ms/step\n",
      "546/546 [==============================] - 2s 4ms/step\n",
      "546/546 [==============================] - 2s 4ms/step\n",
      "546/546 [==============================] - 2s 4ms/step\n",
      "546/546 [==============================] - 2s 5ms/step\n",
      "546/546 [==============================] - 2s 4ms/step\n",
      "546/546 [==============================] - 2s 4ms/step\n",
      "546/546 [==============================] - 2s 4ms/step\n",
      "546/546 [==============================] - 2s 4ms/step\n",
      "546/546 [==============================] - 2s 4ms/step\n",
      "546/546 [==============================] - 2s 4ms/step\n",
      "546/546 [==============================] - 2s 4ms/step\n",
      "546/546 [==============================] - 2s 4ms/step\n",
      "546/546 [==============================] - 2s 4ms/step\n",
      "546/546 [==============================] - 2s 4ms/step\n",
      "546/546 [==============================] - 2s 4ms/step\n",
      "546/546 [==============================] - 2s 4ms/step\n",
      "546/546 [==============================] - 2s 4ms/step\n",
      "546/546 [==============================] - 2s 4ms/step\n",
      "546/546 [==============================] - 2s 4ms/step\n",
      "546/546 [==============================] - 2s 4ms/step\n",
      "546/546 [==============================] - 2s 4ms/step\n",
      "546/546 [==============================] - 2s 4ms/step\n",
      "546/546 [==============================] - 2s 4ms/step\n",
      "546/546 [==============================] - 2s 4ms/step\n",
      "546/546 [==============================] - 2s 4ms/step\n",
      "546/546 [==============================] - 2s 4ms/step\n",
      "546/546 [==============================] - 2s 4ms/step\n",
      "546/546 [==============================] - 2s 4ms/step\n",
      "546/546 [==============================] - 2s 4ms/step\n",
      "546/546 [==============================] - 2s 4ms/step\n",
      "545/545 [==============================] - 3s 5ms/step\n",
      "545/545 [==============================] - 2s 5ms/step\n",
      "545/545 [==============================] - 3s 5ms/step\n",
      "545/545 [==============================] - 2s 4ms/step\n",
      "545/545 [==============================] - 2s 5ms/step\n",
      "545/545 [==============================] - 2s 5ms/step\n",
      "545/545 [==============================] - 2s 4ms/step\n",
      "545/545 [==============================] - 2s 4ms/step\n",
      "545/545 [==============================] - 3s 5ms/step\n",
      "545/545 [==============================] - 2s 5ms/step\n",
      "545/545 [==============================] - 3s 5ms/step\n",
      "545/545 [==============================] - 2s 4ms/step\n",
      "545/545 [==============================] - 3s 5ms/step\n",
      "545/545 [==============================] - 2s 4ms/step\n",
      "545/545 [==============================] - 2s 4ms/step\n",
      "545/545 [==============================] - 2s 4ms/step\n",
      "545/545 [==============================] - 2s 4ms/step\n",
      "545/545 [==============================] - 2s 4ms/step\n",
      "545/545 [==============================] - 2s 4ms/step\n",
      "545/545 [==============================] - 2s 4ms/step\n",
      "545/545 [==============================] - 2s 4ms/step\n",
      "545/545 [==============================] - 2s 4ms/step\n",
      "545/545 [==============================] - 2s 4ms/step\n",
      "545/545 [==============================] - 2s 4ms/step\n",
      "545/545 [==============================] - 2s 4ms/step\n",
      "545/545 [==============================] - 2s 4ms/step\n",
      "545/545 [==============================] - 2s 4ms/step\n",
      "545/545 [==============================] - 2s 4ms/step\n",
      "545/545 [==============================] - 2s 4ms/step\n",
      "545/545 [==============================] - 2s 4ms/step\n",
      "545/545 [==============================] - 2s 4ms/step\n",
      "545/545 [==============================] - 2s 4ms/step\n",
      "545/545 [==============================] - 2s 4ms/step\n",
      "545/545 [==============================] - 3s 5ms/step\n",
      "545/545 [==============================] - 2s 4ms/step\n",
      "545/545 [==============================] - 3s 5ms/step\n",
      "545/545 [==============================] - 2s 4ms/step\n",
      "545/545 [==============================] - 2s 4ms/step\n",
      "545/545 [==============================] - 3s 5ms/step\n",
      "545/545 [==============================] - 3s 5ms/step\n",
      "545/545 [==============================] - 3s 5ms/step\n",
      "545/545 [==============================] - 2s 4ms/step\n",
      "545/545 [==============================] - 3s 5ms/step\n",
      "545/545 [==============================] - 2s 4ms/step\n",
      "545/545 [==============================] - 2s 4ms/step\n",
      "545/545 [==============================] - 2s 4ms/step\n",
      "545/545 [==============================] - 2s 4ms/step\n",
      "545/545 [==============================] - 2s 4ms/step\n",
      "545/545 [==============================] - 3s 4ms/step\n",
      "545/545 [==============================] - 2s 4ms/step\n",
      "545/545 [==============================] - 2s 4ms/step\n",
      "545/545 [==============================] - 2s 4ms/step\n",
      "545/545 [==============================] - 2s 4ms/step\n",
      "545/545 [==============================] - 2s 4ms/step\n",
      "545/545 [==============================] - 2s 4ms/step\n",
      "545/545 [==============================] - 2s 4ms/step\n",
      "545/545 [==============================] - 2s 4ms/step\n",
      "545/545 [==============================] - 2s 4ms/step\n",
      "545/545 [==============================] - 2s 4ms/step\n",
      "545/545 [==============================] - 2s 4ms/step\n",
      "545/545 [==============================] - 2s 4ms/step\n",
      "545/545 [==============================] - 2s 4ms/step\n",
      "545/545 [==============================] - 2s 4ms/step\n",
      "545/545 [==============================] - 2s 4ms/step\n",
      "545/545 [==============================] - 2s 4ms/step\n",
      "545/545 [==============================] - 2s 4ms/step\n",
      "545/545 [==============================] - 2s 4ms/step\n",
      "545/545 [==============================] - 2s 5ms/step\n",
      "545/545 [==============================] - 2s 4ms/step\n",
      "545/545 [==============================] - 2s 4ms/step\n",
      "545/545 [==============================] - 2s 4ms/step\n",
      "545/545 [==============================] - 2s 4ms/step\n",
      "545/545 [==============================] - 3s 5ms/step\n",
      "545/545 [==============================] - 2s 4ms/step\n",
      "545/545 [==============================] - 2s 4ms/step\n",
      "545/545 [==============================] - 2s 4ms/step\n",
      "545/545 [==============================] - 2s 4ms/step\n",
      "545/545 [==============================] - 2s 4ms/step\n",
      "545/545 [==============================] - 2s 4ms/step\n",
      "545/545 [==============================] - 2s 4ms/step\n",
      "545/545 [==============================] - 2s 4ms/step\n",
      "545/545 [==============================] - 2s 4ms/step\n",
      "545/545 [==============================] - 2s 4ms/step\n",
      "545/545 [==============================] - 2s 4ms/step\n",
      "545/545 [==============================] - 2s 4ms/step\n",
      "545/545 [==============================] - 2s 4ms/step\n",
      "545/545 [==============================] - 2s 4ms/step\n",
      "545/545 [==============================] - 2s 4ms/step\n",
      "545/545 [==============================] - 2s 4ms/step\n",
      "545/545 [==============================] - 2s 4ms/step\n",
      "545/545 [==============================] - 3s 5ms/step\n",
      "545/545 [==============================] - 3s 5ms/step\n",
      "545/545 [==============================] - 3s 5ms/step\n",
      "545/545 [==============================] - 3s 5ms/step\n",
      "545/545 [==============================] - 3s 5ms/step\n",
      "545/545 [==============================] - 3s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "directories = os.listdir('models')\n",
    "\n",
    "norm = [model for model in directories if '11' in model]\n",
    "\n",
    "results = []\n",
    "\n",
    "for model_path in norm:\n",
    "    params = model_path.strip('.h5').split('_')\n",
    "    params[2] = 1\n",
    "    total_time = 48\n",
    "    if '01' in params[-1]:\n",
    "        norm_min, norm_max = 0, 1\n",
    "    else:\n",
    "        norm_min, norm_max = -1, 1\n",
    "    df_test = df_initial[df_initial['date'] >= '2019-01-01'].copy()\n",
    "\n",
    "    for col in cols:\n",
    "        df_test[col] = ((norm_max - norm_min) * (df_test[col] - df_initial[col].min()) / (df_initial[col].max() - df_initial[col].min())) + norm_min\n",
    "    \n",
    "    test_X = []\n",
    "    test_Y = []\n",
    "    for i in range(int(params[1]), len(df_test) - total_time):\n",
    "        test_X.append(df_test.iloc[i - int(params[1]):i][input_vars].values)\n",
    "        test_Y.append(df_test.iloc[i:i + total_time][cols].values)\n",
    "    test_X = np.array(test_X)\n",
    "    test_Y = np.array(test_Y)\n",
    "\n",
    "    model = keras.models.load_model(f'models/{model_path}')\n",
    "    y_pred = model.predict(test_X)\n",
    "    \n",
    "    new_day, new_time = np.array((test_X[:, -1, 5] + (1/365)) % 1).reshape(test_X.shape[0], 1, 1), np.array((test_X[:, -1, 6] + (1/24)) % 1).reshape(test_X.shape[0], 1, 1)\n",
    "    y_pred = y_pred.reshape(y_pred.shape[0], 1, y_pred.shape[1])\n",
    "    new_info = np.concatenate((y_pred, new_day, new_time), axis=2)\n",
    "    new_X = np.concatenate((test_X[:, 1:, :], new_info), axis=1)\n",
    "\n",
    "    for idx, col in enumerate(cols):\n",
    "            y_pred[:, :, idx] = ((y_pred[:, :, idx] - norm_min) * (df_initial[col].max() - df_initial[col].min()) / (norm_max - norm_min)) + df_initial[col].min()\n",
    "    results.append(y_pred)\n",
    "\n",
    "    for i in range(0, total_time - 1):\n",
    "        y_pred = model.predict(new_X)\n",
    "\n",
    "        new_day, new_time = np.array((new_X[:, -1, 5] + (1/365)) % 1).reshape(new_X.shape[0], 1, 1), np.array((new_X[:, -1, 6] + (1/24)) % 1).reshape(new_X.shape[0], 1, 1)\n",
    "        y_pred = y_pred.reshape(y_pred.shape[0], 1, y_pred.shape[1])\n",
    "        new_info = np.concatenate((y_pred, new_day, new_time), axis=2)\n",
    "        new_X = np.concatenate((new_X[:, 1:, :], new_info), axis=1)\n",
    "        for idx, col in enumerate(cols):\n",
    "            y_pred[:, :, idx] = ((y_pred[:, :, idx] - norm_min) * (df_initial[col].max() - df_initial[col].min()) / (norm_max - norm_min)) + df_initial[col].min()\n",
    "\n",
    "        results.append(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17448, 48, 5)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = [np.concatenate(results, axis=1)]\n",
    "\n",
    "for idx, col in enumerate(cols):\n",
    "    test_Y[:, :, idx] = ((test_Y[:, :, idx] - norm_min) * (df_initial[col].max() - df_initial[col].min()) / (norm_max - norm_min)) + df_initial[col].min()\n",
    "    \n",
    "rmse = np.sqrt(np.mean(((y_pred - test_Y) ** 2), axis=0))\n",
    "test = [test_Y[:, idx, :] for idx in range(test_Y.shape[1])]\n",
    "\n",
    "s1 = results[0].shape[1]\n",
    "for idx, col in enumerate(cols):\n",
    "    # plot each column\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(test[0][s1:96+s1, idx], label='Real', color='blue')\n",
    "    for idx2, result in enumerate(results):\n",
    "        for i in range(1, result.shape[1], 10):\n",
    "            params = norm[idx2].strip('.h5').split('_')\n",
    "            rmse = np.sqrt(np.mean(np.power((test[0][:, idx] - result[:, i, idx]), 2)))\n",
    "            plt.plot(result[s1-i:96+(s1-i), i, idx], label=f'{i*30} Minutes: {rmse:.4f}')\n",
    "    plt.legend()\n",
    "    plt.title(f'{col} - Batch size - {params[-1]} Normalization')\n",
    "    plt.savefig(f'plots/{col}_batchSize_{params[-1]}norm.png')\n",
    "    plt.clf()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Only 1 output at a given hour\n",
    "This last method will predict one output for, for example, hour 8 and will ignore the other ones that lead to this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_width = [48]\n",
    "prediction_width = [1]\n",
    "batch_size = [6]\n",
    "epochs = [10]\n",
    "dropout = [0.05]\n",
    "neurons = [64]\n",
    "optimizer = ['adagrad']\n",
    "normalization = [[-1, 1]]#, [0, 1]]\n",
    "\n",
    "station = 'CL.zip'\n",
    "variables = ['T', 'HR', 'P', 'u10', 'v10', 'day', 'date', 'time'] # 'time' 'day'\n",
    "input_vars = ['T', 'HR', 'P', 'u10', 'v10', 'day', 'time'] # 'time'\n",
    "cols = ['T', 'HR', 'P', 'u10', 'v10']\n",
    "\n",
    "df_initial = pd.read_csv(f'data/data_by_station/{station}', compression='zip', header=0, sep=',')\n",
    "df_initial['date'] = pd.to_datetime(df_initial['date'], format='%Y-%m-%d %H:%M:%S')\n",
    "df_initial['day'] = df_initial['date'].dt.dayofyear / 365\n",
    "df_initial['time'] = df_initial['date'].dt.hour / 24\n",
    "df_initial = df_initial.astype({'T': 'float', 'HR': 'float', 'P': 'float', 'u2': 'float', 'v2': 'float', 'u6': 'float', 'v6': 'float', 'u10': 'float', 'v10': 'float', 'altitud': 'float', 'latitud': 'float', 'longitud': 'float'})\n",
    "df_initial = df_initial[variables]\n",
    "\n",
    "parameters = []\n",
    "for i in input_width:\n",
    "    for j in prediction_width:\n",
    "        for k in batch_size:\n",
    "            for l in epochs:\n",
    "                for m in dropout:\n",
    "                    for n in neurons:\n",
    "                            for p in optimizer:\n",
    "                                for s in normalization:\n",
    "                                    parameters.append({'width':i, 'output': j, 'batch': k, 'epochs': l, 'dropout': m, 'neurons': n, 'opt': p, 'norm': s})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "23654/23654 [==============================] - 217s 9ms/step - loss: 0.0247 - mae: 0.1095 - val_loss: 0.0140 - val_mae: 0.0776\n",
      "Epoch 2/10\n",
      "23654/23654 [==============================] - 204s 9ms/step - loss: 0.0148 - mae: 0.0835 - val_loss: 0.0121 - val_mae: 0.0719\n",
      "Epoch 3/10\n",
      "23654/23654 [==============================] - 204s 9ms/step - loss: 0.0137 - mae: 0.0799 - val_loss: 0.0114 - val_mae: 0.0693\n",
      "Epoch 4/10\n",
      "23654/23654 [==============================] - 204s 9ms/step - loss: 0.0131 - mae: 0.0776 - val_loss: 0.0110 - val_mae: 0.0676\n",
      "Epoch 5/10\n",
      "23654/23654 [==============================] - 203s 9ms/step - loss: 0.0127 - mae: 0.0760 - val_loss: 0.0107 - val_mae: 0.0664\n",
      "Epoch 6/10\n",
      "23654/23654 [==============================] - 203s 9ms/step - loss: 0.0124 - mae: 0.0748 - val_loss: 0.0105 - val_mae: 0.0654\n",
      "Epoch 7/10\n",
      "23654/23654 [==============================] - 203s 9ms/step - loss: 0.0122 - mae: 0.0738 - val_loss: 0.0103 - val_mae: 0.0645\n",
      "Epoch 8/10\n",
      "23654/23654 [==============================] - 205s 9ms/step - loss: 0.0120 - mae: 0.0729 - val_loss: 0.0102 - val_mae: 0.0639\n",
      "Epoch 9/10\n",
      "23654/23654 [==============================] - 219s 9ms/step - loss: 0.0118 - mae: 0.0722 - val_loss: 0.0101 - val_mae: 0.0633\n",
      "Epoch 10/10\n",
      "23654/23654 [==============================] - 220s 9ms/step - loss: 0.0117 - mae: 0.0716 - val_loss: 0.0100 - val_mae: 0.0628\n"
     ]
    }
   ],
   "source": [
    "last_params = {'width': 0, 'norm': []}\n",
    "for params in parameters:\n",
    "    df = df_initial.copy()\n",
    "    for col in cols:\n",
    "        df[col] = ((params['norm'][1] - params['norm'][0]) * (df[col] - df_initial[col].min()) / (df_initial[col].max() - df_initial[col].min())) + params['norm'][0]\n",
    "\n",
    "    df_train = df[(df['date'] < '2019-01-01')].copy()\n",
    "    df_test = df[df['date'] >= '2019-01-01'].copy()\n",
    "\n",
    "    train_X = []\n",
    "    train_Y = []\n",
    "    for i in range(params['width'], len(df_train) - params['output']):\n",
    "        train_X.append(df_train.iloc[i - params['width']:i][input_vars].values)\n",
    "        train_Y.append(df_train.iloc[i + params['output'] - 1:i + params['output']][cols].values)\n",
    "    train_X = np.array(train_X)\n",
    "    train_Y = np.array(train_Y)\n",
    "\n",
    "    test_X = []\n",
    "    test_Y = []\n",
    "    for i in range(params['width'], len(df_test) - params['output']):\n",
    "        test_X.append(df_test.iloc[i - params['width']:i][input_vars].values)\n",
    "        test_Y.append(df_test.iloc[i + params['output'] - 1:i + params['output']][cols].values)\n",
    "    test_X = np.array(test_X)\n",
    "    test_Y = np.array(test_Y)\n",
    "        \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(params['neurons'], activation='tanh', input_shape=(train_X.shape[1], train_X.shape[2]), return_sequences=False))\n",
    "    model.add(Dropout(params['dropout']))\n",
    "    model.add(Dense(units=len(cols), activation='linear'))\n",
    "    model.compile(optimizer=params['opt'], loss='mse', metrics=['mae'])\n",
    "\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        history = model.fit(train_X, train_Y, epochs=params['epochs'], batch_size=params['batch'], validation_split=0.1, verbose=1, shuffle=False)\n",
    "    model.save(f'models/{station.strip(\".zip\")}_{params[\"width\"]}_{params[\"output\"]}_{params[\"batch\"]}_{params[\"epochs\"]}_{params[\"dropout\"]}_{params[\"neurons\"]}_{params[\"opt\"]}_{params[\"norm\"][0]}{params[\"norm\"][1]}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_width = [48]\n",
    "prediction_width = [1]\n",
    "batch_size = [6]\n",
    "epochs = [10]\n",
    "dropout = [0.05]\n",
    "neurons = [64]\n",
    "optimizer = ['adagrad']\n",
    "normalization = [[-1, 1]]#, [0, 1]]\n",
    "\n",
    "station = 'D6.zip'\n",
    "variables = ['T', 'HR', 'P', 'u10', 'v10', 'day', 'date', 'time'] # 'time' 'day'\n",
    "input_vars = ['T', 'HR', 'P', 'u10', 'v10', 'day', 'time'] # 'time'\n",
    "cols = ['T', 'HR', 'P', 'u10', 'v10']\n",
    "\n",
    "df_initial = pd.read_csv(f'data/data_by_station/{station}', compression='zip', header=0, sep=',')\n",
    "df_initial['date'] = pd.to_datetime(df_initial['date'], format='%Y-%m-%d %H:%M:%S')\n",
    "df_initial['day'] = df_initial['date'].dt.dayofyear / 365\n",
    "df_initial['time'] = df_initial['date'].dt.hour / 24\n",
    "df_initial = df_initial.astype({'T': 'float', 'HR': 'float', 'P': 'float', 'u2': 'float', 'v2': 'float', 'u6': 'float', 'v6': 'float', 'u10': 'float', 'v10': 'float', 'altitud': 'float', 'latitud': 'float', 'longitud': 'float'})\n",
    "df_initial = df_initial[variables]\n",
    "\n",
    "parameters = []\n",
    "for i in input_width:\n",
    "    for j in prediction_width:\n",
    "        for k in batch_size:\n",
    "            for l in epochs:\n",
    "                for m in dropout:\n",
    "                    for n in neurons:\n",
    "                            for p in optimizer:\n",
    "                                for s in normalization:\n",
    "                                    parameters.append({'width':i, 'output': j, 'batch': k, 'epochs': l, 'dropout': m, 'neurons': n, 'opt': p, 'norm': s})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "23656/23656 [==============================] - 232s 10ms/step - loss: 0.0197 - mae: 0.1004 - val_loss: 0.0104 - val_mae: 0.0715\n",
      "Epoch 2/10\n",
      "23656/23656 [==============================] - 212s 9ms/step - loss: 0.0106 - mae: 0.0727 - val_loss: 0.0078 - val_mae: 0.0576\n",
      "Epoch 3/10\n",
      "23656/23656 [==============================] - 213s 9ms/step - loss: 0.0094 - mae: 0.0678 - val_loss: 0.0072 - val_mae: 0.0547\n",
      "Epoch 4/10\n",
      "23656/23656 [==============================] - 212s 9ms/step - loss: 0.0088 - mae: 0.0655 - val_loss: 0.0069 - val_mae: 0.0529\n",
      "Epoch 5/10\n",
      "23656/23656 [==============================] - 212s 9ms/step - loss: 0.0085 - mae: 0.0639 - val_loss: 0.0066 - val_mae: 0.0517\n",
      "Epoch 6/10\n",
      "23656/23656 [==============================] - 213s 9ms/step - loss: 0.0082 - mae: 0.0626 - val_loss: 0.0065 - val_mae: 0.0508\n",
      "Epoch 7/10\n",
      "23656/23656 [==============================] - 213s 9ms/step - loss: 0.0080 - mae: 0.0616 - val_loss: 0.0064 - val_mae: 0.0500\n",
      "Epoch 8/10\n",
      "23656/23656 [==============================] - 214s 9ms/step - loss: 0.0079 - mae: 0.0609 - val_loss: 0.0063 - val_mae: 0.0494\n",
      "Epoch 9/10\n",
      "23656/23656 [==============================] - 215s 9ms/step - loss: 0.0077 - mae: 0.0602 - val_loss: 0.0062 - val_mae: 0.0489\n",
      "Epoch 10/10\n",
      "23656/23656 [==============================] - 223s 9ms/step - loss: 0.0076 - mae: 0.0596 - val_loss: 0.0061 - val_mae: 0.0485\n"
     ]
    }
   ],
   "source": [
    "last_params = {'width': 0, 'norm': []}\n",
    "for params in parameters:\n",
    "    df = df_initial.copy()\n",
    "    for col in cols:\n",
    "        df[col] = ((params['norm'][1] - params['norm'][0]) * (df[col] - df_initial[col].min()) / (df_initial[col].max() - df_initial[col].min())) + params['norm'][0]\n",
    "\n",
    "    df_train = df[(df['date'] < '2019-01-01')].copy()\n",
    "    df_test = df[df['date'] >= '2019-01-01'].copy()\n",
    "\n",
    "    train_X = []\n",
    "    train_Y = []\n",
    "    for i in range(params['width'], len(df_train) - params['output']):\n",
    "        train_X.append(df_train.iloc[i - params['width']:i][input_vars].values)\n",
    "        train_Y.append(df_train.iloc[i + params['output'] - 1:i + params['output']][cols].values)\n",
    "    train_X = np.array(train_X)\n",
    "    train_Y = np.array(train_Y)\n",
    "\n",
    "    test_X = []\n",
    "    test_Y = []\n",
    "    for i in range(params['width'], len(df_test) - params['output']):\n",
    "        test_X.append(df_test.iloc[i - params['width']:i][input_vars].values)\n",
    "        test_Y.append(df_test.iloc[i + params['output'] - 1:i + params['output']][cols].values)\n",
    "    test_X = np.array(test_X)\n",
    "    test_Y = np.array(test_Y)\n",
    "        \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(params['neurons'], activation='tanh', input_shape=(train_X.shape[1], train_X.shape[2]), return_sequences=False))\n",
    "    model.add(Dropout(params['dropout']))\n",
    "    model.add(Dense(units=len(cols), activation='linear'))\n",
    "    model.compile(optimizer=params['opt'], loss='mse', metrics=['mae'])\n",
    "\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        history = model.fit(train_X, train_Y, epochs=params['epochs'], batch_size=params['batch'], validation_split=0.1, verbose=1, shuffle=False)\n",
    "    model.save(f'models/{station.strip(\".zip\")}_{params[\"width\"]}_{params[\"output\"]}_{params[\"batch\"]}_{params[\"epochs\"]}_{params[\"dropout\"]}_{params[\"neurons\"]}_{params[\"opt\"]}_{params[\"norm\"][0]}{params[\"norm\"][1]}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "546/546 [==============================] - 2s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "directories = os.listdir('models')\n",
    "\n",
    "norm = [model for model in directories if '11' in model]\n",
    "\n",
    "results = []\n",
    "\n",
    "for model_path in norm:\n",
    "    params = model_path.strip('.h5').split('_')\n",
    "    if '01' in params[-1]:\n",
    "        norm_min, norm_max = 0, 1\n",
    "    else:\n",
    "        norm_min, norm_max = -1, 1\n",
    "    df_test = df_initial[df_initial['date'] >= '2019-01-01'].copy()\n",
    "\n",
    "    for col in cols:\n",
    "        df_test[col] = ((norm_max - norm_min) * (df_test[col] - df_initial[col].min()) / (df_initial[col].max() - df_initial[col].min())) + norm_min\n",
    "    \n",
    "    test_X = []\n",
    "    test_Y = []\n",
    "    for i in range(int(params[1]), len(df_test) - int(params[2])):\n",
    "        test_X.append(df_test.iloc[i - int(params[1]):i][input_vars].values)\n",
    "        test_Y.append(df_test.iloc[i+ int(params[2]) - 1:i + int(params[2])][cols].values)\n",
    "    test_X = np.array(test_X)\n",
    "    test_Y = np.array(test_Y)\n",
    "\n",
    "    model = keras.models.load_model(f'models/{model_path}')\n",
    "    y_pred = model.predict(test_X)\n",
    "    for idx, col in enumerate(cols):\n",
    "        y_pred[:, idx] = ((y_pred[:, idx] - norm_min) * (df_initial[col].max() - df_initial[col].min()) / (norm_max - norm_min)) + df_initial[col].min()\n",
    "    \n",
    "    results.append(y_pred)\n",
    "\n",
    "for idx, col in enumerate(cols):\n",
    "    test_Y[:, 0, idx] = ((test_Y[:, 0, idx] - norm_min) * (df_initial[col].max() - df_initial[col].min()) / (norm_max - norm_min)) + df_initial[col].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "for idx, col in enumerate(cols):\n",
    "    # plot each column\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(test_Y[:480, :, idx], label='Real', color='blue')\n",
    "    for idx2, result in enumerate(results):\n",
    "        params = norm[idx2].strip('.h5').split('_')\n",
    "        rmse = np.sqrt(np.mean(np.power((test_Y[:, 0, idx] - result[:, idx]), 2)))\n",
    "        plt.plot(result[:480, idx], label=f'{params[3]} Batch size: {rmse:.4f}')\n",
    "    plt.legend()\n",
    "    plt.title(f'{col} at {int(params[2]) * 30} min horizon')\n",
    "    plt.savefig(f'plots/{params[0]}_{col}_{params[2]}_notd.png')\n",
    "    plt.clf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'16'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
