{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = ['data_by_station', 'data_years', 'norm_data_01', 'norm_data_11', 'norm_data_o01', 'norm_data_o11']\n",
    "\n",
    "for dir in dirs:\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Compress data into year long zip files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2009\n",
      "2010\n",
      "2011\n",
      "2012\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2016\n",
      "2017\n",
      "2018\n",
      "2019\n",
      "2020\n",
      "2021\n",
      "2022\n",
      "2023\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "data_years = {}\n",
    "\n",
    "for file in os.listdir('data/stations_data/'):\n",
    "    if file.endswith(\".csv\"):\n",
    "        year = file.strip('.csv')[:4]\n",
    "        if year not in data_years:\n",
    "            data_years[year] = []\n",
    "        data_years[year].append(file)\n",
    "\n",
    "for year in data_years:\n",
    "    print(year)\n",
    "    if int(year) >= 2020:\n",
    "        continue\n",
    "    \n",
    "    data = []\n",
    "    for file in data_years[year]:\n",
    "        df = pd.read_csv('data/stations_data/' + file, sep=',')\n",
    "        data.append(df)\n",
    "    data = pd.concat(data)\n",
    "    data.to_csv('data/data_years/' + year + '.zip', index=False, compression='zip')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find which stations do not have any wind related data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MQ', 'W3', 'MV', 'XZ', 'MS', 'KP', 'XB', 'WE', 'WF', 'W8', 'VY', 'XO', 'XH', 'XQ', 'VZ', 'Y4', 'UO', 'W2', 'WO', 'WW', 'VN', 'X2', 'W1', 'UI', 'V4', 'D2', 'UD', 'VX', 'U8', 'YM', 'UE', 'D1', 'UK'}\n"
     ]
    }
   ],
   "source": [
    "no_data_stations = []\n",
    "\n",
    "for file in os.listdir('data/data_years/'):\n",
    "    df = pd.read_csv('data/data_years/'+file, compression='zip')\n",
    "    for station in df['station'].unique():\n",
    "        df_station = df[df['station'] == station].copy()\n",
    "        num_minus_ones_col = (df_station[['vel10', 'vel6', 'vel2']] == -1).sum() / df_station.shape[0]\n",
    "\n",
    "        if num_minus_ones_col['vel10'] == 1 and num_minus_ones_col['vel6'] == 1 and num_minus_ones_col['vel2'] == 1:\n",
    "            no_data_stations.append(station)\n",
    "\n",
    "print(set(no_data_stations))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Convert data to station data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = True\n",
    "for file in os.listdir('data/data_years/'):\n",
    "    df = pd.read_csv('data/data_years/'+file, compression='zip', header=0, sep=',')\n",
    "    for station in df['station'].unique():\n",
    "        if station not in no_data_stations:\n",
    "            df_station = df[df['station'] == station].copy()\n",
    "\n",
    "            if not os.path.isfile('data/data_by_station/' + station + '.csv'):\n",
    "                df_station.to_csv('data/data_by_station/' + station + '.csv', index=False)\n",
    "            else:\n",
    "                df_station.to_csv('data/data_by_station/' + station + '.csv', mode='a', index=False, header=h)\n",
    "    h = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Convert the winds from (speed, angle) to (speed_x, speed_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "for file in os.listdir('data/data_by_station/'):\n",
    "    if file.endswith(\".csv\"):\n",
    "        df = pd.read_csv('data/data_by_station/'+file)\n",
    "\n",
    "        df = df[df['date'] != 'date'].copy()\t\n",
    "        df = df[(df['date'] >= '2010-01-01') & (df['date'] <= '2020-01-01')].copy()\n",
    "        df.loc[df['HR'] <= 0, 'HR'] = 0  \n",
    "        df.loc[df['P'] <= 0, 'P'] = 0  \n",
    "\n",
    "        df['u2'], df['v2'] = [0, 0]\n",
    "        df['u6'], df['v6'] = [0, 0]\n",
    "        df['u10'], df['v10'] = [0, 0]\n",
    "\n",
    "        df.loc[df['vel2'] != -1, 'u2'] = round(df.loc[df['vel2'] != -1, 'vel2'] * (df.loc[df['vel2'] != -1, 'ang2'] * (math.pi / 180)).apply(math.cos), 1)\n",
    "        df.loc[df['vel2'] != -1, 'v2'] = round(df.loc[df['vel2'] != -1, 'vel2'] * (df.loc[df['vel2'] != -1, 'ang2'] * (math.pi / 180)).apply(math.sin), 1)\n",
    "\n",
    "        df.loc[df['vel6'] != -1, 'u6'] = round(df.loc[df['vel6'] != -1, 'vel6'] * (df.loc[df['vel6'] != -1, 'ang6'] * (math.pi / 180)).apply(math.cos), 1)\n",
    "        df.loc[df['vel6'] != -1, 'v6'] = round(df.loc[df['vel6'] != -1, 'vel6'] * (df.loc[df['vel6'] != -1, 'ang6'] * (math.pi / 180)).apply(math.sin), 1)\n",
    "\n",
    "        df.loc[df['vel10'] != -1, 'u10'] = round(df.loc[df['vel10'] != -1, 'vel10'] * (df.loc[df['vel10'] != -1, 'ang10'] * (math.pi / 180)).apply(math.cos), 1)\n",
    "        df.loc[df['vel10'] != -1, 'v10'] = round(df.loc[df['vel10'] != -1, 'vel10'] * (df.loc[df['vel10'] != -1, 'ang10'] * (math.pi / 180)).apply(math.sin), 1)\n",
    "\n",
    "        df = df.drop(columns=['vel2', 'ang2', 'vel6', 'ang6', 'vel10', 'ang10'])\n",
    "        \n",
    "        df.to_csv('data/data_by_station/'+file.strip('.csv')+'.zip', index=False, compression='zip')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Find the maxs and mins to normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_maxs = {}\n",
    "\n",
    "for file in os.listdir('data/data_by_station/'):\n",
    "    if file.endswith(\".zip\"):\n",
    "        df = pd.read_csv('data/data_by_station/'+file, compression='zip')\n",
    "        for col in df.columns:\n",
    "            if col not in ['date', 'station', 'HR']:\n",
    "                if df[col].unique().shape[0] == 1:\n",
    "                    continue\n",
    "                if col not in min_maxs:\n",
    "                    min_maxs[col] = [df[df[col] != 0][col].min(), df[df[col] != -1][col].max()]\n",
    "                else:\n",
    "                    min_maxs[col][0] = min(min_maxs[col][0], df[df[col] != 0][col].min())\n",
    "                    min_maxs[col][1] = max(min_maxs[col][1], df[df[col] != 0][col].max())\n",
    "import json\n",
    "with open('data/min_maxs.json', 'w') as fp:\n",
    "    json.dump(min_maxs, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir('data/data_by_station/'):\n",
    "    if file.endswith(\".zip\"):\n",
    "        df = pd.read_csv('data/data_by_station/'+file, compression='zip')\n",
    "        df_01 = df.copy()\n",
    "        df_11 = df.copy()\n",
    "\n",
    "        for col in df.columns:\n",
    "            if col not in ['date', 'station', 'HR', 'altitud', 'latitud', 'longitud']:\n",
    "                df_01[col] = (df_01[col] - min_maxs[col][0]) / (min_maxs[col][1] - min_maxs[col][0])\n",
    "                df_01.loc[df_01[col] < 0, col] = 0\n",
    "\n",
    "                df_11[col] = (df_11[col] - min_maxs[col][0]) / (min_maxs[col][1] - min_maxs[col][0]) * 2 - 1\n",
    "                df_11.loc[df_11[col] < -1, col] = -1\n",
    "            if col == 'HR':\n",
    "                df_01[col] = df_01[col] / 100\n",
    "                df_01.loc[df_01[col] < 0, col] = 0\n",
    "\n",
    "                df_11[col] = df_11[col] / 100 \n",
    "                df_11.loc[df_11[col] < 0, col] = 0\n",
    "\n",
    "        df_01.to_csv('data/norm_data_01/'+file.strip('.zip')+'_01.zip', index=False, compression='zip')\n",
    "        df_11.to_csv('data/norm_data_11/'+file.strip('.zip')+'_11.zip', index=False, compression='zip')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Compute the IQR and treat outliers, then normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = {}\n",
    "\n",
    "df = pd.read_csv('data/data_by_station/C6.zip', compression='zip')\n",
    "cols = df.columns\n",
    "\n",
    "for col in cols:\n",
    "    if col in ['date', 'station', 'HR', 'altitud', 'latitud', 'longitud']:\n",
    "        continue\n",
    "\n",
    "    data = []\n",
    "    for file in os.listdir('data/data_by_station/'):\n",
    "        if file.endswith(\".zip\"):\n",
    "            df = pd.read_csv('data/data_by_station/'+file, compression='zip')\n",
    "            if df[col].unique().shape[0] == 1:\n",
    "                continue\n",
    "            \n",
    "            data.append(df[df[col] != 0.0][col].copy())\n",
    "    data = pd.concat(data)\n",
    "    quantiles[col] = [data.quantile(0.25), data.quantile(0.75)]\n",
    "\n",
    "import json\n",
    "\n",
    "for i in quantiles.keys():\n",
    "    iqr = quantiles[i][1] - quantiles[i][0]\n",
    "    quantiles[i].append(quantiles[i][0] - 1.5 * iqr)\n",
    "    quantiles[i].append(quantiles[i][1] + 1.5 * iqr)\n",
    "\n",
    "with open('data/quantiles.json', 'w') as fp:\n",
    "    json.dump(quantiles, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir('data/data_by_station/'):\n",
    "    if file.endswith(\".zip\"):\n",
    "        df = pd.read_csv('data/data_by_station/'+file, compression='zip')\n",
    "        df_01 = df.copy()\n",
    "        df_11 = df.copy()\n",
    "\n",
    "        for col in df.columns:\n",
    "            \n",
    "            if col not in ['date', 'station', 'HR', 'altitud', 'latitud', 'longitud']:\n",
    "                df_01[col] = (df_01[col] - quantiles[col][2]) / (quantiles[col][3] - quantiles[col][2])\n",
    "                df_01.loc[df_01[col] < 0, col] = 0\n",
    "                df_01.loc[df_01[col] > 1, col] = 1\n",
    "\n",
    "\n",
    "                df_11[col] = (df_11[col] - quantiles[col][2]) / (quantiles[col][3] - quantiles[col][2]) * 2 - 1\n",
    "                df_11.loc[df_11[col] < -1, col] = -1\n",
    "                df_11.loc[df_11[col] > 1, col] = 1\n",
    "            if col == 'HR':\n",
    "                df_01[col] = df_01[col] / 100\n",
    "                df_01.loc[df_01[col] < 0, col] = 0\n",
    "\n",
    "                df_11[col] = df_11[col] / 100 \n",
    "                df_11.loc[df_11[col] < 0, col] = 0\n",
    "\n",
    "        df_01.to_csv('data/norm_data_01/'+file.strip('.zip')+'_01.zip', index=False, compression='zip')\n",
    "        df_11.to_csv('data/norm_data_11/'+file.strip('.zip')+'_11.zip', index=False, compression='zip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
