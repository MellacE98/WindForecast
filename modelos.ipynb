{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", tf.config.list_physical_devices())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. No outlier treatment\n",
    "Modelos entrenados sin tratamiento de outliers, hay dos casos:\n",
    "- Datos normalizados entre 0 y 1\n",
    "- Datos normalizados entre -1 y 1\n",
    "\n",
    "Las imagenes en la carpeta plots se identifican de la siguiente forma:\n",
    "\n",
    "{normalización}\\_{variable}\\_{neuronas}\\_{ventana temporal}\\_{epochs}ep_{batch size}bs.png\n",
    "\n",
    "Por ejemplo: **01_T_64_24_15ep_32bs.png**\n",
    "- Los datos son normalizados entre 0 y 1. \n",
    "- La variable que muestra la grafica es la Temperatura (T).\n",
    "- La capa LSTM tiene 64 neuronas.\n",
    "- La ventana temporal es de 24 (24 datos previos al instante predicho, cada una de estas 24 representa una medición cada 30min)\n",
    "- Entrenado durante 15 epochs\n",
    "- Batch size de 32.\n",
    "\n",
    "## 1.1. Training phase\n",
    "In here the training parameters for the sesion are decided. A list of dicts that contains the parameters of each model is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_width = [24]\n",
    "prediction_width = [1]\n",
    "batch_size = [8]\n",
    "epochs = [15]\n",
    "dropout = [0.05]\n",
    "neurons = [64]\n",
    "optimizer = ['adam']\n",
    "normalization = [[-1, 1]]#, [0, 1]]\n",
    "\n",
    "station = 'C6.zip'\n",
    "variables = ['T', 'HR', 'P', 'u10', 'v10', 'day', 'time', 'date']\n",
    "input_vars = ['T', 'HR', 'P', 'u10', 'v10', 'day', 'time']\n",
    "cols = ['T', 'HR', 'P', 'u10', 'v10']\n",
    "\n",
    "df_initial = pd.read_csv(f'data/data_by_station/{station}', compression='zip', header=0, sep=',')\n",
    "df_initial['date'] = pd.to_datetime(df_initial['date'], format='%Y-%m-%d %H:%M:%S')\n",
    "df_initial['day'] = df_initial['date'].dt.dayofyear / 365\n",
    "df_initial['time'] = df_initial['date'].dt.hour / 24\n",
    "df_initial = df_initial.astype({'T': 'float', 'HR': 'float', 'P': 'float', 'u2': 'float', 'v2': 'float', 'u6': 'float', 'v6': 'float', 'u10': 'float', 'v10': 'float', 'altitud': 'float', 'latitud': 'float', 'longitud': 'float'})\n",
    "df_initial = df_initial[variables]\n",
    "\n",
    "parameters = []\n",
    "for i in input_width:\n",
    "    for j in prediction_width:\n",
    "        for k in batch_size:\n",
    "            for l in epochs:\n",
    "                for m in dropout:\n",
    "                    for n in neurons:\n",
    "                            for p in optimizer:\n",
    "                                for s in normalization:\n",
    "                                    parameters.append({'width':i, 'output': j, 'batch': k, 'epochs': l, 'dropout': m, 'neurons': n, 'opt': p, 'norm': s})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "17747/17747 [==============================] - 151s 8ms/step - loss: 0.0248 - mae: 0.1027 - val_loss: 0.0258 - val_mae: 0.1075\n",
      "Epoch 2/15\n",
      "17747/17747 [==============================] - 146s 8ms/step - loss: 0.0227 - mae: 0.0967 - val_loss: 0.0274 - val_mae: 0.1111\n",
      "Epoch 3/15\n",
      "17747/17747 [==============================] - 148s 8ms/step - loss: 0.0220 - mae: 0.0951 - val_loss: 0.0254 - val_mae: 0.1060\n",
      "Epoch 4/15\n",
      "17747/17747 [==============================] - 147s 8ms/step - loss: 0.0216 - mae: 0.0939 - val_loss: 0.0247 - val_mae: 0.1036\n",
      "Epoch 5/15\n",
      "17747/17747 [==============================] - 148s 8ms/step - loss: 0.0212 - mae: 0.0930 - val_loss: 0.0241 - val_mae: 0.1023\n",
      "Epoch 6/15\n",
      "17747/17747 [==============================] - 148s 8ms/step - loss: 0.0209 - mae: 0.0923 - val_loss: 0.0236 - val_mae: 0.1007\n",
      "Epoch 7/15\n",
      "17747/17747 [==============================] - 149s 8ms/step - loss: 0.0207 - mae: 0.0916 - val_loss: 0.0232 - val_mae: 0.0990\n",
      "Epoch 8/15\n",
      "17747/17747 [==============================] - 148s 8ms/step - loss: 0.0204 - mae: 0.0910 - val_loss: 0.0231 - val_mae: 0.0995\n",
      "Epoch 9/15\n",
      "17747/17747 [==============================] - 147s 8ms/step - loss: 0.0202 - mae: 0.0904 - val_loss: 0.0233 - val_mae: 0.1000\n",
      "Epoch 10/15\n",
      "17747/17747 [==============================] - 148s 8ms/step - loss: 0.0200 - mae: 0.0900 - val_loss: 0.0236 - val_mae: 0.1002\n",
      "Epoch 11/15\n",
      "17747/17747 [==============================] - 148s 8ms/step - loss: 0.0199 - mae: 0.0896 - val_loss: 0.0242 - val_mae: 0.1019\n",
      "Epoch 12/15\n",
      "17747/17747 [==============================] - 147s 8ms/step - loss: 0.0197 - mae: 0.0893 - val_loss: 0.0237 - val_mae: 0.1001\n",
      "Epoch 13/15\n",
      "17747/17747 [==============================] - 149s 8ms/step - loss: 0.0196 - mae: 0.0890 - val_loss: 0.0238 - val_mae: 0.1002\n",
      "Epoch 14/15\n",
      "17747/17747 [==============================] - 148s 8ms/step - loss: 0.0196 - mae: 0.0888 - val_loss: 0.0238 - val_mae: 0.1003\n",
      "Epoch 15/15\n",
      "17747/17747 [==============================] - 148s 8ms/step - loss: 0.0195 - mae: 0.0886 - val_loss: 0.0235 - val_mae: 0.0999\n"
     ]
    }
   ],
   "source": [
    "last_params = {'width': 0, 'norm': []}\n",
    "min_maxs = {}\n",
    "\n",
    "for params in parameters:\n",
    "    df = df_initial.copy()\n",
    "    for col in cols:\n",
    "        iqr = df_initial[col].quantile(0.75) - df_initial[col].quantile(0.25)\n",
    "        min_maxs[col] = [df_initial[col].quantile(0.25) - 1.5 * iqr, df_initial[col].quantile(0.75) + 1.5 * iqr]\n",
    "        df[col] = ((params['norm'][1] - params['norm'][0]) * (df[col] - min_maxs[col][0]) / (min_maxs[col][1] - min_maxs[col][0])) + params['norm'][0]\n",
    "        df.loc[df[col] < params['norm'][0], col] = params['norm'][0]\n",
    "        df.loc[df[col] > params['norm'][1], col] = params['norm'][1]\n",
    "        \n",
    "    df_train = df[df['date'] < '2019-01-01'].copy()\n",
    "    df_test = df[df['date'] >= '2019-01-01'].copy()\n",
    "\n",
    "    train_X = []\n",
    "    train_Y = []\n",
    "    for i in range(params['width'], len(df_train) - params['output']):\n",
    "        train_X.append(df_train.iloc[i - params['width']:i][input_vars].values)\n",
    "        train_Y.append(df_train.iloc[i:i + params['output']][cols].values)\n",
    "    train_X = np.array(train_X)\n",
    "    train_Y = np.array(train_Y)\n",
    "\n",
    "    test_X = []\n",
    "    test_Y = []\n",
    "    for i in range(params['width'], len(df_test) - params['output']):\n",
    "        test_X.append(df_test.iloc[i - params['width']:i][input_vars].values)\n",
    "        test_Y.append(df_test.iloc[i:i + params['output']][cols].values)\n",
    "    test_X = np.array(test_X)\n",
    "    test_Y = np.array(test_Y)\n",
    "        \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(params['neurons'], activation='tanh', input_shape=(train_X.shape[1], train_X.shape[2]), return_sequences=False))\n",
    "    model.add(Dropout(params['dropout']))\n",
    "    model.add(Dense(units=len(cols), activation='linear'))\n",
    "    model.compile(optimizer=params['opt'], loss='mse', metrics=['mae'])\n",
    "\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        history = model.fit(train_X, train_Y, epochs=params['epochs'], batch_size=params['batch'], validation_split=0.1, verbose=1, shuffle=False)\n",
    "    model.save(f'models/{station.strip(\".zip\")}_{params[\"width\"]}_{params[\"output\"]}_{params[\"batch\"]}_{params[\"epochs\"]}_{params[\"dropout\"]}_{params[\"neurons\"]}_{params[\"opt\"]}_{params[\"norm\"][0]}{params[\"norm\"][1]}.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Prediction and plotting phase \n",
    "In this phase the predictions of the models trained are done and saved to plot afterwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "547/547 [==============================] - 2s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "directories = os.listdir('models')\n",
    "\n",
    "norm = [model for model in directories if '11' in model]\n",
    "\n",
    "results = []\n",
    "\n",
    "for model_path in norm:\n",
    "    params = model_path.strip('.h5').split('_')\n",
    "    if '01' in params[-1]:\n",
    "        norm_min, norm_max = 0, 1\n",
    "    else:\n",
    "        norm_min, norm_max = -1, 1\n",
    "    df_test = df_initial[df_initial['date'] >= '2019-01-01'].copy()\n",
    "\n",
    "    for col in cols:\n",
    "        iqr = df_initial[col].quantile(0.75) - df_initial[col].quantile(0.25)\n",
    "        min_maxs[col] = [df_initial[col].quantile(0.25) - 1.5 * iqr, df_initial[col].quantile(0.75) + 1.5 * iqr]\n",
    "        df_test[col] = ((norm_max - norm_min) * (df_test[col] - min_maxs[col][0]) / (min_maxs[col][1] - min_maxs[col][0])) + norm_min\n",
    "        df_test.loc[df_test[col] < norm_min, col] = norm_min\n",
    "        df_test.loc[df_test[col] > norm_max, col] = norm_max\n",
    "    \n",
    "    test_X = []\n",
    "    test_Y = []\n",
    "    for i in range(int(params[1]), len(df_test) - int(params[2])):\n",
    "        test_X.append(df_test.iloc[i - int(params[1]):i][input_vars].values)\n",
    "        test_Y.append(df_test.iloc[i:i + int(params[2])][cols].values)\n",
    "    test_X = np.array(test_X)\n",
    "    test_Y = np.array(test_Y)\n",
    "\n",
    "    model = keras.models.load_model(f'models/{model_path}')\n",
    "    y_pred = model.predict(test_X)\n",
    "    for idx, col in enumerate(cols):\n",
    "        y_pred[:, idx] = ((y_pred[:, idx] - norm_min) * (min_maxs[col][1] - min_maxs[col][0]) / (norm_max - norm_min)) + min_maxs[col][0]\n",
    "    \n",
    "    results.append(y_pred)\n",
    "\n",
    "for idx, col in enumerate(cols):\n",
    "    test_Y[:, 0, idx] = ((test_Y[:, 0, idx] - norm_min) * (min_maxs[col][1] - min_maxs[col][0]) / (norm_max - norm_min)) + min_maxs[col][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "minimum = np.inf\n",
    "for idx, result in enumerate(results):\n",
    "    if len(result) < minimum:\n",
    "        minimum = len(result)\n",
    "for idx, result in enumerate(results):\n",
    "    results[idx] = result[-minimum:]\n",
    "test_Y = test_Y[-minimum:, 0, :]\n",
    "\n",
    "for idx, col in enumerate(cols):\n",
    "    # plot each column\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(test_Y[:480, idx], label='Real', color='blue')\n",
    "    for idx2, result in enumerate(results):\n",
    "        params = norm[idx2].strip('.h5').split('_')\n",
    "        rmse = np.sqrt(np.mean(np.power((test_Y[:, idx] - result[:, idx]), 2)))\n",
    "        plt.plot(result[:480, idx], label=f'RMSE {rmse:.4f}')\n",
    "    plt.legend()\n",
    "    plt.title(f'{col} - Drop out - {params[-1]} Normalization (No outliers)')\n",
    "    plt.savefig(f'plots/{col}_dropOut_o{params[-1]}norm.png')\n",
    "    plt.clf()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Treated outliers\n",
    "\n",
    "Modelos entrenados usando IQR como tratamiento de outliers, hay dos casos:\n",
    "- Datos normalizados entre 0 y 1\n",
    "- Datos normalizados entre -1 y 1\n",
    "\n",
    "Las imagenes en la carpeta plots se identifican de la siguiente forma:\n",
    "\n",
    "{normalización}\\_{variable}\\_{neuronas}\\_{ventana temporal}\\_{epochs}ep_{batch size}bs.png\n",
    "\n",
    "Por ejemplo: **o01_T_64_24_15ep_32bs.png**\n",
    "- Los datos son normalizados entre 0 y 1, la letra o indica que se han tratado outliers. \n",
    "- La variable que muestra la grafica es la Temperatura (T).\n",
    "- La capa LSTM tiene 64 neuronas.\n",
    "- La ventana temporal es de 24 (24 datos previos al instante predicho, cada una de estas 24 representa una medición cada 30min)\n",
    "- Entrenado durante 15 epochs\n",
    "- Batch size de 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_width = [24]\n",
    "prediction_width = [1]\n",
    "batch_size = [8]\n",
    "epochs = [20]\n",
    "dropout = [0.05, 0.1, 0.15, 0.3]\n",
    "neurons = [64]\n",
    "optimizer = ['adam']\n",
    "normalization = [[-1, 1]]#, [0, 1]]\n",
    "\n",
    "station = 'C6.zip'\n",
    "variables = ['T', 'HR', 'P', 'u10', 'v10', 'day', 'time', 'date']\n",
    "input_vars = ['T', 'HR', 'P', 'u10', 'v10', 'day', 'time']\n",
    "cols = ['T', 'HR', 'P', 'u10', 'v10']\n",
    "\n",
    "df_initial = pd.read_csv(f'data/data_by_station/{station}', compression='zip', header=0, sep=',')\n",
    "df_initial['date'] = pd.to_datetime(df_initial['date'], format='%Y-%m-%d %H:%M:%S')\n",
    "df_initial['day'] = df_initial['date'].dt.dayofyear / 365\n",
    "df_initial['time'] = df_initial['date'].dt.hour / 24\n",
    "df_initial = df_initial.astype({'T': 'float', 'HR': 'float', 'P': 'float', 'u2': 'float', 'v2': 'float', 'u6': 'float', 'v6': 'float', 'u10': 'float', 'v10': 'float', 'altitud': 'float', 'latitud': 'float', 'longitud': 'float'})\n",
    "df_initial = df_initial[variables]\n",
    "\n",
    "parameters = []\n",
    "for i in input_width:\n",
    "    for j in prediction_width:\n",
    "        for k in batch_size:\n",
    "            for l in epochs:\n",
    "                for m in dropout:\n",
    "                    for n in neurons:\n",
    "                            for p in optimizer:\n",
    "                                for s in normalization:\n",
    "                                    parameters.append({'width':i, 'output': j, 'batch': k, 'epochs': l, 'dropout': m, 'neurons': n, 'opt': p, 'norm': s})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "17747/17747 [==============================] - 144s 8ms/step - loss: 0.0133 - mae: 0.0763 - val_loss: 0.0147 - val_mae: 0.0820\n",
      "Epoch 2/20\n",
      "17747/17747 [==============================] - 138s 8ms/step - loss: 0.0112 - mae: 0.0696 - val_loss: 0.0170 - val_mae: 0.0846\n",
      "Epoch 3/20\n",
      "17747/17747 [==============================] - 137s 8ms/step - loss: 0.0107 - mae: 0.0680 - val_loss: 0.0139 - val_mae: 0.0785\n",
      "Epoch 4/20\n",
      "17747/17747 [==============================] - 138s 8ms/step - loss: 0.0103 - mae: 0.0668 - val_loss: 0.0130 - val_mae: 0.0759\n",
      "Epoch 5/20\n",
      "17747/17747 [==============================] - 139s 8ms/step - loss: 0.0100 - mae: 0.0659 - val_loss: 0.0131 - val_mae: 0.0758\n",
      "Epoch 6/20\n",
      "17747/17747 [==============================] - 138s 8ms/step - loss: 0.0098 - mae: 0.0652 - val_loss: 0.0128 - val_mae: 0.0746\n",
      "Epoch 7/20\n",
      "17747/17747 [==============================] - 139s 8ms/step - loss: 0.0097 - mae: 0.0647 - val_loss: 0.0127 - val_mae: 0.0745\n",
      "Epoch 8/20\n",
      "17747/17747 [==============================] - 138s 8ms/step - loss: 0.0096 - mae: 0.0644 - val_loss: 0.0123 - val_mae: 0.0726\n",
      "Epoch 9/20\n",
      "17747/17747 [==============================] - 139s 8ms/step - loss: 0.0095 - mae: 0.0640 - val_loss: 0.0130 - val_mae: 0.0739\n",
      "Epoch 10/20\n",
      "17747/17747 [==============================] - 139s 8ms/step - loss: 0.0094 - mae: 0.0638 - val_loss: 0.0125 - val_mae: 0.0726\n",
      "Epoch 11/20\n",
      "17747/17747 [==============================] - 139s 8ms/step - loss: 0.0094 - mae: 0.0636 - val_loss: 0.0125 - val_mae: 0.0719\n",
      "Epoch 12/20\n",
      "17747/17747 [==============================] - 139s 8ms/step - loss: 0.0093 - mae: 0.0634 - val_loss: 0.0124 - val_mae: 0.0718\n",
      "Epoch 13/20\n",
      "17747/17747 [==============================] - 139s 8ms/step - loss: 0.0093 - mae: 0.0633 - val_loss: 0.0130 - val_mae: 0.0731\n",
      "Epoch 14/20\n",
      "17747/17747 [==============================] - 140s 8ms/step - loss: 0.0093 - mae: 0.0632 - val_loss: 0.0129 - val_mae: 0.0728\n",
      "Epoch 15/20\n",
      "17747/17747 [==============================] - 139s 8ms/step - loss: 0.0093 - mae: 0.0631 - val_loss: 0.0127 - val_mae: 0.0729\n",
      "Epoch 16/20\n",
      "17747/17747 [==============================] - 139s 8ms/step - loss: 0.0092 - mae: 0.0630 - val_loss: 0.0127 - val_mae: 0.0723\n",
      "Epoch 17/20\n",
      "17747/17747 [==============================] - 141s 8ms/step - loss: 0.0092 - mae: 0.0629 - val_loss: 0.0128 - val_mae: 0.0720\n",
      "Epoch 18/20\n",
      "17747/17747 [==============================] - 139s 8ms/step - loss: 0.0092 - mae: 0.0628 - val_loss: 0.0130 - val_mae: 0.0728\n",
      "Epoch 19/20\n",
      "17747/17747 [==============================] - 139s 8ms/step - loss: 0.0092 - mae: 0.0628 - val_loss: 0.0131 - val_mae: 0.0727\n",
      "Epoch 20/20\n",
      "17747/17747 [==============================] - 137s 8ms/step - loss: 0.0092 - mae: 0.0627 - val_loss: 0.0129 - val_mae: 0.0725\n",
      "Epoch 1/20\n",
      "17747/17747 [==============================] - 140s 8ms/step - loss: 0.0138 - mae: 0.0782 - val_loss: 0.0142 - val_mae: 0.0800\n",
      "Epoch 2/20\n",
      "17747/17747 [==============================] - 135s 8ms/step - loss: 0.0118 - mae: 0.0722 - val_loss: 0.0171 - val_mae: 0.0849\n",
      "Epoch 3/20\n",
      "17747/17747 [==============================] - 134s 8ms/step - loss: 0.0111 - mae: 0.0703 - val_loss: 0.0145 - val_mae: 0.0803\n",
      "Epoch 4/20\n",
      "17747/17747 [==============================] - 133s 8ms/step - loss: 0.0107 - mae: 0.0691 - val_loss: 0.0137 - val_mae: 0.0782\n",
      "Epoch 5/20\n",
      "17747/17747 [==============================] - 136s 8ms/step - loss: 0.0104 - mae: 0.0682 - val_loss: 0.0138 - val_mae: 0.0777\n",
      "Epoch 6/20\n",
      "17747/17747 [==============================] - 134s 8ms/step - loss: 0.0102 - mae: 0.0676 - val_loss: 0.0140 - val_mae: 0.0777\n",
      "Epoch 7/20\n",
      "17747/17747 [==============================] - 136s 8ms/step - loss: 0.0101 - mae: 0.0671 - val_loss: 0.0142 - val_mae: 0.0787\n",
      "Epoch 8/20\n",
      "17747/17747 [==============================] - 133s 8ms/step - loss: 0.0100 - mae: 0.0667 - val_loss: 0.0137 - val_mae: 0.0762\n",
      "Epoch 9/20\n",
      "17747/17747 [==============================] - 134s 8ms/step - loss: 0.0099 - mae: 0.0664 - val_loss: 0.0140 - val_mae: 0.0773\n",
      "Epoch 10/20\n",
      "17747/17747 [==============================] - 137s 8ms/step - loss: 0.0098 - mae: 0.0661 - val_loss: 0.0140 - val_mae: 0.0772\n",
      "Epoch 11/20\n",
      "17747/17747 [==============================] - 134s 8ms/step - loss: 0.0098 - mae: 0.0659 - val_loss: 0.0136 - val_mae: 0.0765\n",
      "Epoch 12/20\n",
      "17747/17747 [==============================] - 136s 8ms/step - loss: 0.0097 - mae: 0.0658 - val_loss: 0.0137 - val_mae: 0.0768\n",
      "Epoch 13/20\n",
      "17747/17747 [==============================] - 133s 8ms/step - loss: 0.0097 - mae: 0.0656 - val_loss: 0.0143 - val_mae: 0.0781\n",
      "Epoch 14/20\n",
      "17747/17747 [==============================] - 133s 8ms/step - loss: 0.0097 - mae: 0.0655 - val_loss: 0.0141 - val_mae: 0.0779\n",
      "Epoch 15/20\n",
      "17747/17747 [==============================] - 134s 8ms/step - loss: 0.0097 - mae: 0.0654 - val_loss: 0.0141 - val_mae: 0.0776\n",
      "Epoch 16/20\n",
      "17747/17747 [==============================] - 136s 8ms/step - loss: 0.0096 - mae: 0.0653 - val_loss: 0.0138 - val_mae: 0.0769\n",
      "Epoch 17/20\n",
      "17747/17747 [==============================] - 135s 8ms/step - loss: 0.0096 - mae: 0.0652 - val_loss: 0.0139 - val_mae: 0.0763\n",
      "Epoch 18/20\n",
      "17747/17747 [==============================] - 135s 8ms/step - loss: 0.0096 - mae: 0.0652 - val_loss: 0.0138 - val_mae: 0.0763\n",
      "Epoch 19/20\n",
      "17747/17747 [==============================] - 136s 8ms/step - loss: 0.0096 - mae: 0.0651 - val_loss: 0.0138 - val_mae: 0.0766\n",
      "Epoch 20/20\n",
      "17747/17747 [==============================] - 136s 8ms/step - loss: 0.0096 - mae: 0.0651 - val_loss: 0.0139 - val_mae: 0.0772\n",
      "Epoch 1/20\n",
      "17747/17747 [==============================] - 142s 8ms/step - loss: 0.0142 - mae: 0.0804 - val_loss: 0.0160 - val_mae: 0.0867\n",
      "Epoch 2/20\n",
      "17747/17747 [==============================] - 139s 8ms/step - loss: 0.0122 - mae: 0.0744 - val_loss: 0.0181 - val_mae: 0.0883\n",
      "Epoch 3/20\n",
      "17747/17747 [==============================] - 140s 8ms/step - loss: 0.0117 - mae: 0.0728 - val_loss: 0.0162 - val_mae: 0.0851\n",
      "Epoch 4/20\n",
      "17747/17747 [==============================] - 140s 8ms/step - loss: 0.0111 - mae: 0.0714 - val_loss: 0.0141 - val_mae: 0.0804\n",
      "Epoch 5/20\n",
      "17747/17747 [==============================] - 140s 8ms/step - loss: 0.0108 - mae: 0.0704 - val_loss: 0.0143 - val_mae: 0.0800\n",
      "Epoch 6/20\n",
      "17747/17747 [==============================] - 139s 8ms/step - loss: 0.0105 - mae: 0.0697 - val_loss: 0.0142 - val_mae: 0.0812\n",
      "Epoch 7/20\n",
      "17747/17747 [==============================] - 137s 8ms/step - loss: 0.0104 - mae: 0.0692 - val_loss: 0.0148 - val_mae: 0.0818\n",
      "Epoch 8/20\n",
      "17747/17747 [==============================] - 138s 8ms/step - loss: 0.0103 - mae: 0.0689 - val_loss: 0.0149 - val_mae: 0.0825\n",
      "Epoch 9/20\n",
      "17747/17747 [==============================] - 138s 8ms/step - loss: 0.0103 - mae: 0.0686 - val_loss: 0.0144 - val_mae: 0.0810\n",
      "Epoch 10/20\n",
      "17747/17747 [==============================] - 137s 8ms/step - loss: 0.0102 - mae: 0.0684 - val_loss: 0.0143 - val_mae: 0.0796\n",
      "Epoch 11/20\n",
      "17747/17747 [==============================] - 136s 8ms/step - loss: 0.0102 - mae: 0.0682 - val_loss: 0.0144 - val_mae: 0.0800\n",
      "Epoch 12/20\n",
      "17747/17747 [==============================] - 137s 8ms/step - loss: 0.0101 - mae: 0.0680 - val_loss: 0.0141 - val_mae: 0.0788\n",
      "Epoch 13/20\n",
      "17747/17747 [==============================] - 137s 8ms/step - loss: 0.0101 - mae: 0.0679 - val_loss: 0.0143 - val_mae: 0.0796\n",
      "Epoch 14/20\n",
      "17747/17747 [==============================] - 137s 8ms/step - loss: 0.0100 - mae: 0.0677 - val_loss: 0.0143 - val_mae: 0.0791\n",
      "Epoch 15/20\n",
      "17747/17747 [==============================] - 137s 8ms/step - loss: 0.0100 - mae: 0.0677 - val_loss: 0.0140 - val_mae: 0.0789\n",
      "Epoch 16/20\n",
      "17747/17747 [==============================] - 137s 8ms/step - loss: 0.0100 - mae: 0.0676 - val_loss: 0.0143 - val_mae: 0.0788\n",
      "Epoch 17/20\n",
      "17747/17747 [==============================] - 137s 8ms/step - loss: 0.0100 - mae: 0.0675 - val_loss: 0.0142 - val_mae: 0.0797\n",
      "Epoch 18/20\n",
      "17747/17747 [==============================] - 138s 8ms/step - loss: 0.0100 - mae: 0.0675 - val_loss: 0.0142 - val_mae: 0.0793\n",
      "Epoch 19/20\n",
      "17747/17747 [==============================] - 137s 8ms/step - loss: 0.0099 - mae: 0.0673 - val_loss: 0.0139 - val_mae: 0.0782\n",
      "Epoch 20/20\n",
      "17747/17747 [==============================] - 137s 8ms/step - loss: 0.0099 - mae: 0.0673 - val_loss: 0.0142 - val_mae: 0.0791\n",
      "Epoch 1/20\n",
      "17747/17747 [==============================] - 144s 8ms/step - loss: 0.0157 - mae: 0.0864 - val_loss: 0.0182 - val_mae: 0.0923\n",
      "Epoch 2/20\n",
      "17747/17747 [==============================] - 142s 8ms/step - loss: 0.0143 - mae: 0.0815 - val_loss: 0.0182 - val_mae: 0.0918\n",
      "Epoch 3/20\n",
      "17747/17747 [==============================] - 141s 8ms/step - loss: 0.0137 - mae: 0.0804 - val_loss: 0.0167 - val_mae: 0.0877\n",
      "Epoch 4/20\n",
      "17747/17747 [==============================] - 140s 8ms/step - loss: 0.0129 - mae: 0.0788 - val_loss: 0.0161 - val_mae: 0.0877\n",
      "Epoch 5/20\n",
      "17747/17747 [==============================] - 142s 8ms/step - loss: 0.0123 - mae: 0.0776 - val_loss: 0.0168 - val_mae: 0.0891\n",
      "Epoch 6/20\n",
      "17747/17747 [==============================] - 142s 8ms/step - loss: 0.0121 - mae: 0.0768 - val_loss: 0.0165 - val_mae: 0.0879\n",
      "Epoch 7/20\n",
      "17747/17747 [==============================] - 142s 8ms/step - loss: 0.0119 - mae: 0.0763 - val_loss: 0.0164 - val_mae: 0.0871\n",
      "Epoch 8/20\n",
      "17747/17747 [==============================] - 142s 8ms/step - loss: 0.0118 - mae: 0.0760 - val_loss: 0.0167 - val_mae: 0.0884\n",
      "Epoch 9/20\n",
      "17747/17747 [==============================] - 142s 8ms/step - loss: 0.0117 - mae: 0.0757 - val_loss: 0.0162 - val_mae: 0.0867\n",
      "Epoch 10/20\n",
      "17747/17747 [==============================] - 140s 8ms/step - loss: 0.0117 - mae: 0.0755 - val_loss: 0.0161 - val_mae: 0.0861\n",
      "Epoch 11/20\n",
      "17747/17747 [==============================] - 141s 8ms/step - loss: 0.0116 - mae: 0.0753 - val_loss: 0.0157 - val_mae: 0.0852\n",
      "Epoch 12/20\n",
      "17747/17747 [==============================] - 141s 8ms/step - loss: 0.0116 - mae: 0.0751 - val_loss: 0.0165 - val_mae: 0.0872\n",
      "Epoch 13/20\n",
      "17747/17747 [==============================] - 142s 8ms/step - loss: 0.0115 - mae: 0.0750 - val_loss: 0.0166 - val_mae: 0.0875\n",
      "Epoch 14/20\n",
      "17747/17747 [==============================] - 142s 8ms/step - loss: 0.0115 - mae: 0.0749 - val_loss: 0.0166 - val_mae: 0.0879\n",
      "Epoch 15/20\n",
      "17747/17747 [==============================] - 141s 8ms/step - loss: 0.0115 - mae: 0.0749 - val_loss: 0.0164 - val_mae: 0.0874\n",
      "Epoch 16/20\n",
      "17747/17747 [==============================] - 141s 8ms/step - loss: 0.0114 - mae: 0.0747 - val_loss: 0.0164 - val_mae: 0.0882\n",
      "Epoch 17/20\n",
      "17747/17747 [==============================] - 142s 8ms/step - loss: 0.0114 - mae: 0.0746 - val_loss: 0.0171 - val_mae: 0.0888\n",
      "Epoch 18/20\n",
      "17747/17747 [==============================] - 140s 8ms/step - loss: 0.0114 - mae: 0.0746 - val_loss: 0.0164 - val_mae: 0.0874\n",
      "Epoch 19/20\n",
      "17747/17747 [==============================] - 142s 8ms/step - loss: 0.0114 - mae: 0.0745 - val_loss: 0.0164 - val_mae: 0.0869\n",
      "Epoch 20/20\n",
      "17747/17747 [==============================] - 140s 8ms/step - loss: 0.0113 - mae: 0.0744 - val_loss: 0.0171 - val_mae: 0.0883\n"
     ]
    }
   ],
   "source": [
    "last_params = {'width': 0, 'norm': []}\n",
    "for params in parameters:\n",
    "    df = df_initial.copy()\n",
    "    for col in cols:\n",
    "        df[col] = ((params['norm'][1] - params['norm'][0]) * (df[col] - df_initial[col].min()) / (df_initial[col].max() - df_initial[col].min())) + params['norm'][0]\n",
    "\n",
    "    df_train = df[df['date'] < '2019-01-01'].copy()\n",
    "    df_test = df[df['date'] >= '2019-01-01'].copy()\n",
    "\n",
    "    train_X = []\n",
    "    train_Y = []\n",
    "    for i in range(params['width'], len(df_train) - params['output']):\n",
    "        train_X.append(df_train.iloc[i - params['width']:i][input_vars].values)\n",
    "        train_Y.append(df_train.iloc[i:i + params['output']][cols].values)\n",
    "    train_X = np.array(train_X)\n",
    "    train_Y = np.array(train_Y)\n",
    "\n",
    "    test_X = []\n",
    "    test_Y = []\n",
    "    for i in range(params['width'], len(df_test) - params['output']):\n",
    "        test_X.append(df_test.iloc[i - params['width']:i][input_vars].values)\n",
    "        test_Y.append(df_test.iloc[i:i + params['output']][cols].values)\n",
    "    test_X = np.array(test_X)\n",
    "    test_Y = np.array(test_Y)\n",
    "        \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(params['neurons'], activation='tanh', input_shape=(train_X.shape[1], train_X.shape[2]), return_sequences=False))\n",
    "    model.add(Dropout(params['dropout']))\n",
    "    model.add(Dense(units=len(cols), activation='linear'))\n",
    "    model.compile(optimizer=params['opt'], loss='mse', metrics=['mae'])\n",
    "\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        history = model.fit(train_X, train_Y, epochs=params['epochs'], batch_size=params['batch'], validation_split=0.1, verbose=1, shuffle=False)\n",
    "    model.save(f'models/{station.strip(\".zip\")}_{params[\"width\"]}_{params[\"output\"]}_{params[\"batch\"]}_{params[\"epochs\"]}_{params[\"dropout\"]}_{params[\"neurons\"]}_{params[\"opt\"]}_{params[\"norm\"][0]}{params[\"norm\"][1]}.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Prediction and plotting phase \n",
    "In this phase the predictions of the models trained are done and saved to plot afterwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "547/547 [==============================] - 10s 4ms/step\n",
      "547/547 [==============================] - 2s 4ms/step\n",
      "547/547 [==============================] - 3s 4ms/step\n",
      "547/547 [==============================] - 2s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "directories = os.listdir('models')\n",
    "\n",
    "norm = [model for model in directories if '11' in model]\n",
    "\n",
    "results = []\n",
    "\n",
    "for model_path in norm:\n",
    "    params = model_path.strip('.h5').split('_')\n",
    "    if '01' in params[-1]:\n",
    "        norm_min, norm_max = 0, 1\n",
    "    else:\n",
    "        norm_min, norm_max = -1, 1\n",
    "    df_test = df_initial[df_initial['date'] >= '2019-01-01'].copy()\n",
    "\n",
    "    for col in cols:\n",
    "        df_test[col] = ((norm_max - norm_min) * (df_test[col] - df_initial[col].min()) / (df_initial[col].max() - df_initial[col].min())) + norm_min\n",
    "    \n",
    "    test_X = []\n",
    "    test_Y = []\n",
    "    for i in range(int(params[1]), len(df_test) - int(params[2])):\n",
    "        test_X.append(df_test.iloc[i - int(params[1]):i][input_vars].values)\n",
    "        test_Y.append(df_test.iloc[i:i + int(params[2])][cols].values)\n",
    "    test_X = np.array(test_X)\n",
    "    test_Y = np.array(test_Y)\n",
    "\n",
    "    model = keras.models.load_model(f'models/{model_path}')\n",
    "    y_pred = model.predict(test_X)\n",
    "    for idx, col in enumerate(cols):\n",
    "        y_pred[:, idx] = ((y_pred[:, idx] - norm_min) * (df_initial[col].max() - df_initial[col].min()) / (norm_max - norm_min)) + df_initial[col].min()\n",
    "    \n",
    "    results.append(y_pred)\n",
    "\n",
    "for idx, col in enumerate(cols):\n",
    "    test_Y[:, 0, idx] = ((test_Y[:, 0, idx] - norm_min) * (df_initial[col].max() - df_initial[col].min()) / (norm_max - norm_min)) + df_initial[col].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "minimum = np.inf\n",
    "for idx, result in enumerate(results):\n",
    "    if len(result) < minimum:\n",
    "        minimum = len(result)\n",
    "for idx, result in enumerate(results):\n",
    "    results[idx] = result[-minimum:]\n",
    "test_Y = test_Y[-minimum:, 0, :]\n",
    "\n",
    "for idx, col in enumerate(cols):\n",
    "    # plot each column\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(test_Y[:480, idx], label='Real', color='blue')\n",
    "    for idx2, result in enumerate(results):\n",
    "        params = norm[idx2].strip('.h5').split('_')\n",
    "        rmse = np.sqrt(np.mean(np.power((test_Y[:, idx] - result[:, idx]), 2)))\n",
    "        plt.plot(result[:480, idx], label=f'{params[3]} Batch size: {rmse:.4f}')\n",
    "    plt.legend()\n",
    "    plt.title(f'{col} - Batch size - {params[-1]} Normalization')\n",
    "    plt.savefig(f'plots/{col}_batchSize_{params[-1]}norm.png')\n",
    "    plt.clf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
